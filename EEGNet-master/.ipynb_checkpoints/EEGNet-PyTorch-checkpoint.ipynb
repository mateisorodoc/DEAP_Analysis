{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWritten by, \\nSriram Ravindran, sriram@ucsd.edu\\n\\nOriginal paper - https://arxiv.org/abs/1611.08024\\n\\nPlease reach out to me if you spot an error.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Written by, \n",
    "Sriram Ravindran, sriram@ucsd.edu\n",
    "\n",
    "Original paper - https://arxiv.org/abs/1611.08024\n",
    "\n",
    "Please reach out to me if you spot an error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:38:06.685418Z",
     "start_time": "2025-10-03T11:38:06.672717Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here's the description from the paper</p>\n",
    "<img src=\"EEGNet.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T12:30:38.301662Z",
     "start_time": "2025-10-03T12:30:38.285760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[EEGNet] Flattened feature dimension: 256\n",
      "Output: tensor([[-0.0215, -0.5169]], grad_fn=<AddmmBackward0>)\n",
      "Criterion & optimizer ready\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, T=120, C=32, dropout=0.4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            T: number of timepoints per segment\n",
    "            C: number of EEG channels (DEAP: 32)\n",
    "            dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = T\n",
    "        self.C = C\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Layer 1: temporal conv across channels\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, C), padding=0)  # kernel spans all channels\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, C//2))  # adjust kernel to half channels\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4)\n",
    "        self.pooling2 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # Dynamically infer flatten size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, T, C)\n",
    "            out = self._forward_features(dummy)\n",
    "            flatten_dim = out.shape[1]\n",
    "        print(f\"[EEGNet] Flattened feature dimension: {flatten_dim}\")\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(flatten_dim, 2)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = x.permute(0, 3, 1, 2)  # rearrange for next conv\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# Usage Example\n",
    "# ------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "net = EEGNet(T=120, C=32, dropout=0.4).to(device)\n",
    "\n",
    "# Example forward pass with DEAP-like input\n",
    "x_dummy = torch.rand(1, 1, 120, 32).to(device)\n",
    "output = net(x_dummy)\n",
    "print(\"Output:\", output)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "print(\"Criterion & optimizer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate function returns values of different criteria like accuracy, precision etc.\n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:43:18.189575Z",
     "start_time": "2025-10-03T11:43:18.177393Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "\n",
    "def evaluate(model, X, Y, params=[\"acc\"], batch_size=100, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate a trained multi-label EEGNet model on given data.\n",
    "\n",
    "    Args:\n",
    "        model: torch.nn.Module\n",
    "        X: numpy array, shape [samples, 1, timepoints, channels]\n",
    "        Y: numpy array, shape [samples, n_labels] (e.g., valence+arousal)\n",
    "        params: list of metrics to compute ['acc', 'auc', 'precision', 'recall', 'fmeasure']\n",
    "        batch_size: batch size for evaluation\n",
    "        device: torch.device (default: cuda if available)\n",
    "\n",
    "    Returns:\n",
    "        results: list of computed metrics (average across labels)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    preds = []\n",
    "\n",
    "    # Iterate over batches\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_x = X[i:i + batch_size]\n",
    "        inputs = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():  # disable gradient computation\n",
    "            output = model(inputs)        # raw logits\n",
    "            output = torch.sigmoid(output)  # convert logits â†’ probabilities\n",
    "\n",
    "        preds.append(output.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    predicted = np.vstack(preds)  # shape: [samples, n_labels]\n",
    "\n",
    "    results = []\n",
    "    for param in params:\n",
    "        if param == \"acc\":\n",
    "            results.append(np.mean(np.round(predicted) == Y))  # average accuracy over labels\n",
    "        elif param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted, average='macro'))\n",
    "        elif param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted), average='macro'))\n",
    "        elif param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted), average='macro'))\n",
    "        elif param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted), average='macro')\n",
    "            recall = recall_score(Y, np.round(predicted), average='macro')\n",
    "            results.append(2 * precision * recall / (precision + recall + 1e-8))  # avoid div0\n",
    "\n",
    "    model.train()  # switch back to training mode\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate random data\n",
    "\n",
    "##### Data format:\n",
    "Datatype - float32 (both X and Y) <br>\n",
    "X.shape - (#samples, 1, #timepoints,  #channels) <br>\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (40, 40, 8064)\n",
      "Labels shape: (40, 4)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Path to your subject file\n",
    "file_path = \"G:\\DEAP\\data_preprocessed_python\\s01.dat\"\n",
    "\n",
    "# Load pickled data\n",
    "with open(file_path, \"rb\") as f:\n",
    "    x = pickle.load(f, encoding=\"latin1\")  # Python 3 version; in Python 2 use cPickle\n",
    "\n",
    "# Extract arrays\n",
    "data = x[\"data\"]      # shape: (40 trials, 40 channels, 8064 timepoints)\n",
    "labels = x[\"labels\"]  # shape: (40 trials, 4 labels: valence, arousal, dominance, liking)\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:43:23.734583Z",
     "start_time": "2025-10-03T11:43:23.702811Z"
    }
   },
   "outputs": [],
   "source": [
    "n_trials, n_channels, n_samples = data.shape\n",
    "\n",
    "# Optional: select first 32 channels for simplicity\n",
    "data = data[:, :32, :]\n",
    "\n",
    "# Define segment length (timepoints your model expects)\n",
    "segment_len = 120\n",
    "n_segments = n_samples // segment_len  # 8064 // 120 = 67 segments per trial\n",
    "\n",
    "# Create X array: [trials*segments, 1, segment_len, channels]\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    for seg in range(n_segments):\n",
    "        start = seg * segment_len\n",
    "        end = start + segment_len\n",
    "        segment = data[trial, :, start:end].T  # shape: (230, 32)\n",
    "        X_list.append(segment[np.newaxis, :, :])\n",
    "\n",
    "        valence = int(labels[trial, 0] > 5.0)\n",
    "        arousal = int(labels[trial, 1] > 5.0)\n",
    "        Y_list.append([valence, arousal])\n",
    "\n",
    "X = np.array(X_list, dtype=\"float32\")\n",
    "Y = np.array(Y_list, dtype=\"float32\")  # shape: (segments, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (924, 1, 240, 32) y_train: (924, 2)\n",
      "X_val: (198, 1, 240, 32) y_val: (198, 2)\n",
      "X_test: (198, 1, 240, 32) y_test: (198, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:43:59.365239Z",
     "start_time": "2025-10-03T11:43:58.522689Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "Training Loss: 0.4894\n",
      "Train     - [np.float64(0.814935064935065), 0.9434323951611185, 0.8202976431262378]\n",
      "Validation- [np.float64(0.5681818181818182), 0.5838208201539716, 0.5286873100243642]\n",
      "Test      - [np.float64(0.5732323232323232), 0.6019098438156605, 0.5337818977552176]\n",
      "\n",
      "Epoch 2/200\n",
      "Training Loss: 0.4918\n",
      "Train     - [np.float64(0.8122294372294372), 0.9472776133846612, 0.7895021836889863]\n",
      "Validation- [np.float64(0.5580808080808081), 0.586487819735892, 0.4493166025817176]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6267815870570108, 0.5142577033148708]\n",
      "\n",
      "Epoch 3/200\n",
      "Training Loss: 0.4741\n",
      "Train     - [np.float64(0.827922077922078), 0.9488093146337826, 0.8381645844009193]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5807853879060968, 0.5468556909797242]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6178058026334221, 0.5902159600670712]\n",
      "\n",
      "Epoch 4/200\n",
      "Training Loss: 0.4885\n",
      "Train     - [np.float64(0.8598484848484849), 0.9539259505516156, 0.8777617611178665]\n",
      "Validation- [np.float64(0.5984848484848485), 0.5944725070418966, 0.6035759751866583]\n",
      "Test      - [np.float64(0.5858585858585859), 0.615129482420507, 0.6066397780574678]\n",
      "\n",
      "Epoch 5/200\n",
      "Training Loss: 0.4750\n",
      "Train     - [np.float64(0.8760822510822511), 0.9545671336229848, 0.8830410147189356]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5863935934576874, 0.5839727853786244]\n",
      "Test      - [np.float64(0.5883838383838383), 0.6124444074800393, 0.5889500074745672]\n",
      "\n",
      "Epoch 6/200\n",
      "Training Loss: 0.4751\n",
      "Train     - [np.float64(0.8409090909090909), 0.9506466073786553, 0.8666961353002058]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5915204267790555, 0.6218599925261236]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6276758824765373, 0.6529564429669192]\n",
      "\n",
      "Epoch 7/200\n",
      "Training Loss: 0.4664\n",
      "Train     - [np.float64(0.8436147186147186), 0.9507924480331396, 0.8718284320247249]\n",
      "Validation- [np.float64(0.5883838383838383), 0.5799519472375072, 0.6283944859283683]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6152975731895223, 0.6664915423343118]\n",
      "\n",
      "Epoch 8/200\n",
      "Training Loss: 0.4781\n",
      "Train     - [np.float64(0.8506493506493507), 0.9567613158371138, 0.8706050869961302]\n",
      "Validation- [np.float64(0.5833333333333334), 0.5717762735592089, 0.6000865283678429]\n",
      "Test      - [np.float64(0.5984848484848485), 0.6175860589718448, 0.620374425073778]\n",
      "\n",
      "Epoch 9/200\n",
      "Training Loss: 0.4651\n",
      "Train     - [np.float64(0.8506493506493507), 0.9612097802589823, 0.8639795197169332]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5859413601101365, 0.5638310886792278]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6087446596161927, 0.5950709511202846]\n",
      "\n",
      "Epoch 10/200\n",
      "Training Loss: 0.4580\n",
      "Train     - [np.float64(0.8241341991341992), 0.9561345920388473, 0.8416018344006958]\n",
      "Validation- [np.float64(0.5656565656565656), 0.6077503885184357, 0.5537249943376461]\n",
      "Test      - [np.float64(0.5909090909090909), 0.6213554069197367, 0.5789399059525219]\n",
      "\n",
      "Epoch 11/200\n",
      "Training Loss: 0.4751\n",
      "Train     - [np.float64(0.8728354978354979), 0.9567477883468576, 0.8800901027444208]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5900691573372974, 0.5866579698663242]\n",
      "Test      - [np.float64(0.5833333333333334), 0.6066991175234627, 0.5922059944747462]\n",
      "\n",
      "Epoch 12/200\n",
      "Training Loss: 0.4861\n",
      "Train     - [np.float64(0.8663419913419913), 0.9551212449151278, 0.8788932214280061]\n",
      "Validation- [np.float64(0.5505050505050505), 0.589256119165361, 0.566942254088661]\n",
      "Test      - [np.float64(0.601010101010101), 0.6101756198347107, 0.6197081125715622]\n",
      "\n",
      "Epoch 13/200\n",
      "Training Loss: 0.4599\n",
      "Train     - [np.float64(0.8733766233766234), 0.9580217850829553, 0.8739673729449583]\n",
      "Validation- [np.float64(0.5732323232323232), 0.601718982926304, 0.549346037083292]\n",
      "Test      - [np.float64(0.5833333333333334), 0.6187802038100574, 0.5597104208141799]\n",
      "\n",
      "Epoch 14/200\n",
      "Training Loss: 0.4619\n",
      "Train     - [np.float64(0.8658008658008658), 0.9571773901561136, 0.8737252799247278]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5970994934479745, 0.5806601117550034]\n",
      "Test      - [np.float64(0.5606060606060606), 0.6107608733716207, 0.5583351077637738]\n",
      "\n",
      "Epoch 15/200\n",
      "Training Loss: 0.4624\n",
      "Train     - [np.float64(0.8847402597402597), 0.9600391282239686, 0.8965581323048496]\n",
      "Validation- [np.float64(0.5580808080808081), 0.5820477824887984, 0.5787522875041269]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6083844025773918, 0.6358107852589938]\n",
      "\n",
      "Epoch 16/200\n",
      "Training Loss: 0.4567\n",
      "Train     - [np.float64(0.8869047619047619), 0.9619580850797607, 0.8903372238496907]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5690712667390212, 0.5660529373702564]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6211597387589298, 0.5967443345765668]\n",
      "\n",
      "Epoch 17/200\n",
      "Training Loss: 0.4671\n",
      "Train     - [np.float64(0.8658008658008658), 0.9612842064138607, 0.8646024495176119]\n",
      "Validation- [np.float64(0.5732323232323232), 0.5842645282668277, 0.5479913539852463]\n",
      "Test      - [np.float64(0.5909090909090909), 0.6208183043843676, 0.568541707329413]\n",
      "\n",
      "Epoch 18/200\n",
      "Training Loss: 0.4421\n",
      "Train     - [np.float64(0.8468614718614719), 0.9625917639714182, 0.8573410657681657]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5877504254699173, 0.571683332780154]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6202930207311947, 0.6033041964980677]\n",
      "\n",
      "Epoch 19/200\n",
      "Training Loss: 0.4639\n",
      "Train     - [np.float64(0.8425324675324676), 0.9619741749728452, 0.8690363444037953]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5910212914437261, 0.6194817882847208]\n",
      "Test      - [np.float64(0.5883838383838383), 0.6132809216977169, 0.6234242092699204]\n",
      "\n",
      "Epoch 20/200\n",
      "Training Loss: 0.4396\n",
      "Train     - [np.float64(0.8344155844155844), 0.9585619695493366, 0.8541381212913132]\n",
      "Validation- [np.float64(0.5883838383838383), 0.5809176214225686, 0.588760651502889]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6203394207872251, 0.6192053483228714]\n",
      "\n",
      "Epoch 21/200\n",
      "Training Loss: 0.4635\n",
      "Train     - [np.float64(0.8446969696969697), 0.9609786981130066, 0.8509981835509184]\n",
      "Validation- [np.float64(0.5782828282828283), 0.6039290774376365, 0.5501633073914023]\n",
      "Test      - [np.float64(0.601010101010101), 0.6264988093570528, 0.5910015899478337]\n",
      "\n",
      "Epoch 22/200\n",
      "Training Loss: 0.4679\n",
      "Train     - [np.float64(0.8836580086580087), 0.9625015141471258, 0.8835010103499312]\n",
      "Validation- [np.float64(0.5631313131313131), 0.5950457828857628, 0.5434519082487451]\n",
      "Test      - [np.float64(0.5858585858585859), 0.6108387904468413, 0.5807347083794485]\n",
      "\n",
      "Epoch 23/200\n",
      "Training Loss: 0.4690\n",
      "Train     - [np.float64(0.8733766233766234), 0.9582933997827614, 0.8803391914086548]\n",
      "Validation- [np.float64(0.5757575757575758), 0.59749020257858, 0.5716975448080115]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6293173938927021, 0.6357356754173663]\n",
      "\n",
      "Epoch 24/200\n",
      "Training Loss: 0.4589\n",
      "Train     - [np.float64(0.8717532467532467), 0.962089216876451, 0.8697369480952087]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5901270127999932, 0.5207500719015]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6259332539571368, 0.5965912210955312]\n",
      "\n",
      "Epoch 25/200\n",
      "Training Loss: 0.4564\n",
      "Train     - [np.float64(0.8798701298701299), 0.9626667392126169, 0.8928003837696933]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5946752387065715, 0.5938593197780901]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6292561107998319, 0.6225527492000604]\n",
      "\n",
      "Epoch 26/200\n",
      "Training Loss: 0.4550\n",
      "Train     - [np.float64(0.8782467532467533), 0.9681968537686624, 0.8918730947221145]\n",
      "Validation- [np.float64(0.5732323232323232), 0.5865717259931502, 0.5791801695067923]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6127744607087828, 0.6171157123035451]\n",
      "\n",
      "Epoch 27/200\n",
      "Training Loss: 0.4271\n",
      "Train     - [np.float64(0.8582251082251082), 0.9627884699859435, 0.8543641535488039]\n",
      "Validation- [np.float64(0.5984848484848485), 0.599433401816746, 0.5664774507241522]\n",
      "Test      - [np.float64(0.5909090909090909), 0.6197734276509315, 0.5589656524810902]\n",
      "\n",
      "Epoch 28/200\n",
      "Training Loss: 0.4473\n",
      "Train     - [np.float64(0.8457792207792207), 0.966402722536366, 0.8635806064463394]\n",
      "Validation- [np.float64(0.601010101010101), 0.6030403415161509, 0.5967043537208846]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6152165919596583, 0.5914981764077362]\n",
      "\n",
      "Epoch 29/200\n",
      "Training Loss: 0.4626\n",
      "Train     - [np.float64(0.8836580086580087), 0.9657946177560539, 0.8854055319799551]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5945280398400317, 0.5477476396568056]\n",
      "Test      - [np.float64(0.5984848484848485), 0.6150069162347668, 0.5946907468490208]\n",
      "\n",
      "Epoch 30/200\n",
      "Training Loss: 0.4344\n",
      "Train     - [np.float64(0.8901515151515151), 0.9679064703000873, 0.8976638180745244]\n",
      "Validation- [np.float64(0.553030303030303), 0.5845989391757497, 0.5518854964395119]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6158027209693235, 0.6245062187550764]\n",
      "\n",
      "Epoch 31/200\n",
      "Training Loss: 0.4531\n",
      "Train     - [np.float64(0.8831168831168831), 0.9649306054724938, 0.8850191222923759]\n",
      "Validation- [np.float64(0.5808080808080808), 0.59423208486594, 0.5646686577606786]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6118153803053649, 0.5935829302479685]\n",
      "\n",
      "Epoch 32/200\n",
      "Training Loss: 0.4308\n",
      "Train     - [np.float64(0.8847402597402597), 0.9695907343513726, 0.896959841912524]\n",
      "Validation- [np.float64(0.5631313131313131), 0.5921752862156193, 0.5656355061039194]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6181459237988514, 0.6324800097015839]\n",
      "\n",
      "Epoch 33/200\n",
      "Training Loss: 0.4336\n",
      "Train     - [np.float64(0.8382034632034632), 0.9698955770930506, 0.8467747089903913]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5971607801196805, 0.554236537792831]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6179458782742682, 0.5987102487435104]\n",
      "\n",
      "Epoch 34/200\n",
      "Training Loss: 0.4520\n",
      "Train     - [np.float64(0.8636363636363636), 0.9670990638244628, 0.8558796089884037]\n",
      "Validation- [np.float64(0.5555555555555556), 0.5856137060435732, 0.4901971718450835]\n",
      "Test      - [np.float64(0.5808080808080808), 0.6088440257739179, 0.526034180094948]\n",
      "\n",
      "Epoch 35/200\n",
      "Training Loss: 0.4504\n",
      "Train     - [np.float64(0.8858225108225108), 0.9689953417513258, 0.8967511119200791]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5930001752555987, 0.5810822081262962]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6181853200728393, 0.6136368125492893]\n",
      "\n",
      "Epoch 36/200\n",
      "Training Loss: 0.4416\n",
      "Train     - [np.float64(0.8587662337662337), 0.9680736587651482, 0.8619740765128098]\n",
      "Validation- [np.float64(0.5732323232323232), 0.5898276266168913, 0.5358543975241019]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6192739704440398, 0.5967147405401215]\n",
      "\n",
      "Epoch 37/200\n",
      "Training Loss: 0.4402\n",
      "Train     - [np.float64(0.8982683982683982), 0.9660312406821716, 0.9010696164598504]\n",
      "Validation- [np.float64(0.5505050505050505), 0.5909367253385811, 0.5434202887135215]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6268813909511136, 0.5949671531581846]\n",
      "\n",
      "Epoch 38/200\n",
      "Training Loss: 0.4449\n",
      "Train     - [np.float64(0.8679653679653679), 0.9684837929685004, 0.8609449350300502]\n",
      "Validation- [np.float64(0.5555555555555556), 0.5796133396960266, 0.4997813057635981]\n",
      "Test      - [np.float64(0.601010101010101), 0.6148003046645188, 0.5663489113514426]\n",
      "\n",
      "Epoch 39/200\n",
      "Training Loss: 0.4495\n",
      "Train     - [np.float64(0.8425324675324676), 0.9665488959704385, 0.8324043739561244]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5838496423096576, 0.5320497265776876]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6220903662978008, 0.538178480964585]\n",
      "\n",
      "Epoch 40/200\n",
      "Training Loss: 0.4557\n",
      "Train     - [np.float64(0.8982683982683982), 0.9681670367175685, 0.9047723559152195]\n",
      "Validation- [np.float64(0.5505050505050505), 0.5677103436698945, 0.5645869527516577]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6203149075500771, 0.6302197587262136]\n",
      "\n",
      "Epoch 41/200\n",
      "Training Loss: 0.4365\n",
      "Train     - [np.float64(0.8820346320346321), 0.9719429535918898, 0.8775228857474058]\n",
      "Validation- [np.float64(0.5580808080808081), 0.5703365382585084, 0.5148021611772732]\n",
      "Test      - [np.float64(0.5909090909090909), 0.6175939382266424, 0.5624316221523071]\n",
      "\n",
      "Epoch 42/200\n",
      "Training Loss: 0.4361\n",
      "Train     - [np.float64(0.8344155844155844), 0.9722110907450003, 0.8576627211678362]\n",
      "Validation- [np.float64(0.5959595959595959), 0.5952867593339444, 0.5991649013454267]\n",
      "Test      - [np.float64(0.5883838383838383), 0.6225320423028435, 0.595039106043339]\n",
      "\n",
      "Epoch 43/200\n",
      "Training Loss: 0.4283\n",
      "Train     - [np.float64(0.8760822510822511), 0.972632373064554, 0.8761259467612823]\n",
      "Validation- [np.float64(0.5833333333333334), 0.5810970736538047, 0.5448466059283852]\n",
      "Test      - [np.float64(0.5808080808080808), 0.6150095426530326, 0.5581095733793647]\n",
      "\n",
      "Epoch 44/200\n",
      "Training Loss: 0.4247\n",
      "Train     - [np.float64(0.8603896103896104), 0.9704970429205801, 0.8733926170056979]\n",
      "Validation- [np.float64(0.5732323232323232), 0.5783543763223351, 0.5602048175344675]\n",
      "Test      - [np.float64(0.601010101010101), 0.6211785614231686, 0.6091504783476924]\n",
      "\n",
      "Epoch 45/200\n",
      "Training Loss: 0.4385\n",
      "Train     - [np.float64(0.8874458874458875), 0.9710195734298128, 0.8970322893113917]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5885212333771122, 0.5744085625401265]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6085861990474857, 0.6244724955736546]\n",
      "\n",
      "Epoch 46/200\n",
      "Training Loss: 0.4276\n",
      "Train     - [np.float64(0.8414502164502164), 0.9704045634730475, 0.8498756900158098]\n",
      "Validation- [np.float64(0.5934343434343434), 0.5941477299121188, 0.5755614042190623]\n",
      "Test      - [np.float64(0.6186868686868687), 0.616414238688892, 0.6156827722633046]\n",
      "\n",
      "Epoch 47/200\n",
      "Training Loss: 0.4198\n",
      "Train     - [np.float64(0.9053030303030303), 0.9700356273827018, 0.9121295728638448]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5916882392935721, 0.5837957204016094]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6254355476957557, 0.6372681499674706]\n",
      "\n",
      "Epoch 48/200\n",
      "Training Loss: 0.4619\n",
      "Train     - [np.float64(0.9025974025974026), 0.973687334275765, 0.9040720587981627]\n",
      "Validation- [np.float64(0.5681818181818182), 0.579994071926587, 0.5536403901299981]\n",
      "Test      - [np.float64(0.5858585858585859), 0.6071005217817622, 0.5793146187119883]\n",
      "\n",
      "Epoch 49/200\n",
      "Training Loss: 0.4499\n",
      "Train     - [np.float64(0.8977272727272727), 0.9750205158616063, 0.8993013392115111]\n",
      "Validation- [np.float64(0.5656565656565656), 0.576415954382868, 0.542377871996416]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6213549691833589, 0.6032930294422741]\n",
      "\n",
      "Epoch 50/200\n",
      "Training Loss: 0.4400\n",
      "Train     - [np.float64(0.8766233766233766), 0.9710437831448469, 0.8834255181042917]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5836837037631389, 0.5578364203233577]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6279420261941449, 0.6034058813711782]\n",
      "\n",
      "Epoch 51/200\n",
      "Training Loss: 0.4287\n",
      "Train     - [np.float64(0.8955627705627706), 0.9737129749430281, 0.8939631699455259]\n",
      "Validation- [np.float64(0.5707070707070707), 0.5764631730976322, 0.5425284705177035]\n",
      "Test      - [np.float64(0.601010101010101), 0.6210362971004343, 0.5901029291889031]\n",
      "\n",
      "Epoch 52/200\n",
      "Training Loss: 0.4226\n",
      "Train     - [np.float64(0.8858225108225108), 0.9706981083477094, 0.8847885960445292]\n",
      "Validation- [np.float64(0.5732323232323232), 0.5961753896797679, 0.5566191167016326]\n",
      "Test      - [np.float64(0.6111111111111112), 0.622367891161227, 0.6022128980034259]\n",
      "\n",
      "Epoch 53/200\n",
      "Training Loss: 0.4252\n",
      "Train     - [np.float64(0.9074675324675324), 0.9721237693810831, 0.9139809109614638]\n",
      "Validation- [np.float64(0.5707070707070707), 0.5859842502227647, 0.5808836215367498]\n",
      "Test      - [np.float64(0.5984848484848485), 0.6034336041462389, 0.6288982559791867]\n",
      "\n",
      "Epoch 54/200\n",
      "Training Loss: 0.4484\n",
      "Train     - [np.float64(0.8836580086580087), 0.9756612330415522, 0.8994577836250259]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5810884164495327, 0.5947350331079013]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6043375297660737, 0.6317630746660938]\n",
      "\n",
      "Epoch 55/200\n",
      "Training Loss: 0.4461\n",
      "Train     - [np.float64(0.8955627705627706), 0.9705423675004792, 0.9034261679443704]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5881374922401889, 0.5968853784906869]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6138775563804455, 0.6206156815298033]\n",
      "\n",
      "Epoch 56/200\n",
      "Training Loss: 0.4230\n",
      "Train     - [np.float64(0.8955627705627706), 0.9736077999542095, 0.8981472937542387]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5815987428050187, 0.5564356013022294]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6095413398235048, 0.6273687351727266]\n",
      "\n",
      "Epoch 57/200\n",
      "Training Loss: 0.4209\n",
      "Train     - [np.float64(0.8533549783549783), 0.973739264530488, 0.8716476610055531]\n",
      "Validation- [np.float64(0.5959595959595959), 0.5992231214922487, 0.5910682904733439]\n",
      "Test      - [np.float64(0.6060606060606061), 0.616902314749965, 0.6117872193592839]\n",
      "\n",
      "Epoch 58/200\n",
      "Training Loss: 0.4408\n",
      "Train     - [np.float64(0.875), 0.972123719464145, 0.8763409338841179]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5933611912313078, 0.5456508047006117]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6221888569827707, 0.5986243824196509]\n",
      "\n",
      "Epoch 59/200\n",
      "Training Loss: 0.4376\n",
      "Train     - [np.float64(0.8484848484848485), 0.9723471476795945, 0.8693000242765003]\n",
      "Validation- [np.float64(0.601010101010101), 0.5944154170027491, 0.6021997243515299]\n",
      "Test      - [np.float64(0.6035353535353535), 0.614669859223981, 0.6163109682945115]\n",
      "\n",
      "Epoch 60/200\n",
      "Training Loss: 0.4353\n",
      "Train     - [np.float64(0.8717532467532467), 0.9691977050188485, 0.8696229231236462]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5935245695680267, 0.5288563927859575]\n",
      "Test      - [np.float64(0.6161616161616161), 0.615175007003782, 0.5957487362051891]\n",
      "\n",
      "Epoch 61/200\n",
      "Training Loss: 0.4428\n",
      "Train     - [np.float64(0.8793290043290043), 0.973905970465146, 0.8880456456383513]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5877277003087032, 0.5667437201968567]\n",
      "Test      - [np.float64(0.601010101010101), 0.6284905098753326, 0.6045009744243446]\n",
      "\n",
      "Epoch 62/200\n",
      "Training Loss: 0.4184\n",
      "Train     - [np.float64(0.8814935064935064), 0.9750170216759313, 0.8772515478582468]\n",
      "Validation- [np.float64(0.5606060606060606), 0.601630193794685, 0.5198747926399039]\n",
      "Test      - [np.float64(0.5808080808080808), 0.6275743276369239, 0.5371787314963483]\n",
      "\n",
      "Epoch 63/200\n",
      "Training Loss: 0.4449\n",
      "Train     - [np.float64(0.8901515151515151), 0.974490847230209, 0.892620602714102]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5835404903778342, 0.5700742627881322]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6132927405799131, 0.5884454653752978]\n",
      "\n",
      "Epoch 64/200\n",
      "Training Loss: 0.4353\n",
      "Train     - [np.float64(0.8944805194805194), 0.9748737101463165, 0.8979458091723247]\n",
      "Validation- [np.float64(0.553030303030303), 0.5918576090491011, 0.5325679880801822]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6175965646449083, 0.5986243824196509]\n",
      "\n",
      "Epoch 65/200\n",
      "Training Loss: 0.4311\n",
      "Train     - [np.float64(0.8863636363636364), 0.9769398886918834, 0.8887656676447715]\n",
      "Validation- [np.float64(0.5631313131313131), 0.5870788322909497, 0.5412413743174864]\n",
      "Test      - [np.float64(0.5959595959595959), 0.5998625507774198, 0.5858800721546064]\n",
      "\n",
      "Epoch 66/200\n",
      "Training Loss: 0.4255\n",
      "Train     - [np.float64(0.8739177489177489), 0.9802271852969991, 0.8788356211162097]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5852249764566273, 0.566258881828122]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6173798851379746, 0.6088353033031891]\n",
      "\n",
      "Epoch 67/200\n",
      "Training Loss: 0.4188\n",
      "Train     - [np.float64(0.8814935064935064), 0.9778016380742445, 0.882792027050802]\n",
      "Validation- [np.float64(0.5782828282828283), 0.6041161839043568, 0.5560096324295554]\n",
      "Test      - [np.float64(0.6111111111111112), 0.6230792127748985, 0.5966059623651864]\n",
      "\n",
      "Epoch 68/200\n",
      "Training Loss: 0.4190\n",
      "Train     - [np.float64(0.8841991341991342), 0.9743996489840905, 0.8892638365955136]\n",
      "Validation- [np.float64(0.5732323232323232), 0.5937122303069717, 0.5610720218008323]\n",
      "Test      - [np.float64(0.6287878787878788), 0.631190905589018, 0.6322421119041325]\n",
      "\n",
      "Epoch 69/200\n",
      "Training Loss: 0.4301\n",
      "Train     - [np.float64(0.8414502164502164), 0.9776451651119205, 0.8391918884268191]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5959039282592262, 0.5417202536116454]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6100613706401457, 0.5527439660949774]\n",
      "\n",
      "Epoch 70/200\n",
      "Training Loss: 0.4234\n",
      "Train     - [np.float64(0.8663419913419913), 0.9767579248131111, 0.8689387937086585]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5994690863904526, 0.5527274545112774]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6123481054769576, 0.5873221525119815]\n",
      "\n",
      "Epoch 71/200\n",
      "Training Loss: 0.4268\n",
      "Train     - [np.float64(0.8636363636363636), 0.9794159185781526, 0.8740148718403897]\n",
      "Validation- [np.float64(0.5959595959595959), 0.6210863630029098, 0.5849389900430979]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6248130865667461, 0.6050406422910087]\n",
      "\n",
      "Epoch 72/200\n",
      "Training Loss: 0.4006\n",
      "Train     - [np.float64(0.8917748917748918), 0.9789649356803611, 0.8994361099105983]\n",
      "Validation- [np.float64(0.5883838383838383), 0.5927846953297551, 0.5852693610392412]\n",
      "Test      - [np.float64(0.6262626262626263), 0.6183604146238969, 0.6270508565824344]\n",
      "\n",
      "Epoch 73/200\n",
      "Training Loss: 0.4318\n",
      "Train     - [np.float64(0.8706709956709957), 0.9780507069569571, 0.8714562386720873]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5999251996435766, 0.5686398442768626]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6173763832469534, 0.582121753451254]\n",
      "\n",
      "Epoch 74/200\n",
      "Training Loss: 0.3992\n",
      "Train     - [np.float64(0.8825757575757576), 0.9755381545108939, 0.8853162787734845]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5865202842519119, 0.5497724431022009]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6178027384787785, 0.6152047898366736]\n",
      "\n",
      "Epoch 75/200\n",
      "Training Loss: 0.4376\n",
      "Train     - [np.float64(0.8787878787878788), 0.9785397099226887, 0.8818494989475458]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5981312052095255, 0.5656233844834546]\n",
      "Test      - [np.float64(0.6237373737373737), 0.616762239109119, 0.5995231851284474]\n",
      "\n",
      "Epoch 76/200\n",
      "Training Loss: 0.4116\n",
      "Train     - [np.float64(0.9004329004329005), 0.9796010438630118, 0.908479850369588]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5847783914069857, 0.5744085625401265]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6253357438016529, 0.6290474500582891]\n",
      "\n",
      "Epoch 77/200\n",
      "Training Loss: 0.4409\n",
      "Train     - [np.float64(0.8647186147186147), 0.9784487779002407, 0.8781404590082801]\n",
      "Validation- [np.float64(0.5782828282828283), 0.602798705220083, 0.5750656116128218]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6202615037120045, 0.6273747074266922]\n",
      "\n",
      "Epoch 78/200\n",
      "Training Loss: 0.4192\n",
      "Train     - [np.float64(0.8852813852813853), 0.9761414007092198, 0.8898206386968458]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5983854841911003, 0.5680810209960039]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6017189907550078, 0.5853747910235558]\n",
      "\n",
      "Epoch 79/200\n",
      "Training Loss: 0.4219\n",
      "Train     - [np.float64(0.9020562770562771), 0.9789695613166358, 0.9075053704043401]\n",
      "Validation- [np.float64(0.5883838383838383), 0.593242920096116, 0.5837795614692369]\n",
      "Test      - [np.float64(0.6060606060606061), 0.5989034703740019, 0.6073348420522091]\n",
      "\n",
      "Epoch 80/200\n",
      "Training Loss: 0.3982\n",
      "Train     - [np.float64(0.8906926406926406), 0.9774610548048048, 0.8934562659534212]\n",
      "Validation- [np.float64(0.553030303030303), 0.590232984370579, 0.5306681684764679]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6036581629079703, 0.6023922540417935]\n",
      "\n",
      "Epoch 81/200\n",
      "Training Loss: 0.4022\n",
      "Train     - [np.float64(0.8885281385281385), 0.9783846179956552, 0.9006625819832275]\n",
      "Validation- [np.float64(0.601010101010101), 0.5889606128878322, 0.6159757951747181]\n",
      "Test      - [np.float64(0.6111111111111112), 0.6023204405378906, 0.6242634383472423]\n",
      "\n",
      "Epoch 82/200\n",
      "Training Loss: 0.4139\n",
      "Train     - [np.float64(0.8690476190476191), 0.9733859857197623, 0.8701488021960113]\n",
      "Validation- [np.float64(0.6060606060606061), 0.6182309372584956, 0.5918861137529073]\n",
      "Test      - [np.float64(0.6262626262626263), 0.6199712844936266, 0.6122655948675992]\n",
      "\n",
      "Epoch 83/200\n",
      "Training Loss: 0.4042\n",
      "Train     - [np.float64(0.8365800865800865), 0.9760422823089047, 0.8259804033091734]\n",
      "Validation- [np.float64(0.5934343434343434), 0.6150083985439005, 0.5493795065053959]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6193912837932483, 0.5646318073363167]\n",
      "\n",
      "Epoch 84/200\n",
      "Training Loss: 0.4172\n",
      "Train     - [np.float64(0.8814935064935064), 0.9787268651630354, 0.8755181556220215]\n",
      "Validation- [np.float64(0.5883838383838383), 0.6020428626629559, 0.5443385619046464]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6325934129429893, 0.5698835808314076]\n",
      "\n",
      "Epoch 85/200\n",
      "Training Loss: 0.4079\n",
      "Train     - [np.float64(0.8820346320346321), 0.9767990730091793, 0.8821924860214457]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5976497010097256, 0.5494907975444002]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6258159406079282, 0.612671719365984]\n",
      "\n",
      "Epoch 86/200\n",
      "Training Loss: 0.4250\n",
      "Train     - [np.float64(0.8825757575757576), 0.9787241363704129, 0.8842840026992375]\n",
      "Validation- [np.float64(0.5959595959595959), 0.5920592321693265, 0.5874726330438091]\n",
      "Test      - [np.float64(0.6414141414141414), 0.6263022657234907, 0.6293042596486824]\n",
      "\n",
      "Epoch 87/200\n",
      "Training Loss: 0.4101\n",
      "Train     - [np.float64(0.8955627705627706), 0.9779181275690585, 0.8932414716223487]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5979888892173465, 0.5479012559456018]\n",
      "Test      - [np.float64(0.6287878787878788), 0.6309956751645889, 0.6113897024026378]\n",
      "\n",
      "Epoch 88/200\n",
      "Training Loss: 0.4009\n",
      "Train     - [np.float64(0.9047619047619048), 0.978866798979831, 0.9035257996362515]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5967157523110512, 0.5411455737470567]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6160544193864688, 0.5935071657391524]\n",
      "\n",
      "Epoch 89/200\n",
      "Training Loss: 0.4284\n",
      "Train     - [np.float64(0.8371212121212122), 0.9759837630183374, 0.8384101935664209]\n",
      "Validation- [np.float64(0.5883838383838383), 0.6023326150669138, 0.5463521188085255]\n",
      "Test      - [np.float64(0.601010101010101), 0.6072738653873091, 0.5707067775542501]\n",
      "\n",
      "Epoch 90/200\n",
      "Training Loss: 0.3967\n",
      "Train     - [np.float64(0.8836580086580087), 0.9795890804368198, 0.8848505441316885]\n",
      "Validation- [np.float64(0.5681818181818182), 0.5886494814123489, 0.5421417897512198]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6118315765513377, 0.5992145910399136]\n",
      "\n",
      "Epoch 91/200\n",
      "Training Loss: 0.4201\n",
      "Train     - [np.float64(0.8701298701298701), 0.979571958927012, 0.8856725905862407]\n",
      "Validation- [np.float64(0.5782828282828283), 0.583763730114824, 0.5757850975203079]\n",
      "Test      - [np.float64(0.6338383838383839), 0.618741245272447, 0.6464443612451767]\n",
      "\n",
      "Epoch 92/200\n",
      "Training Loss: 0.4265\n",
      "Train     - [np.float64(0.9015151515151515), 0.9778285266649628, 0.9019077713479698]\n",
      "Validation- [np.float64(0.547979797979798), 0.5942764266439187, 0.521905864549659]\n",
      "Test      - [np.float64(0.6287878787878788), 0.614593693094271, 0.6227942138067817]\n",
      "\n",
      "Epoch 93/200\n",
      "Training Loss: 0.4077\n",
      "Train     - [np.float64(0.8831168831168831), 0.9801241234585649, 0.8768406372828321]\n",
      "Validation- [np.float64(0.5808080808080808), 0.6033391734270283, 0.5449535866653805]\n",
      "Test      - [np.float64(0.6161616161616161), 0.6131513517299342, 0.5994978010966279]\n",
      "\n",
      "Epoch 94/200\n",
      "Training Loss: 0.4044\n",
      "Train     - [np.float64(0.8614718614718615), 0.9775777273283922, 0.8724333486913755]\n",
      "Validation- [np.float64(0.5934343434343434), 0.5912188235070546, 0.5822303074218547]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6240973875892982, 0.6131977822077612]\n",
      "\n",
      "Epoch 95/200\n",
      "Training Loss: 0.4036\n",
      "Train     - [np.float64(0.8777056277056277), 0.9785577964933018, 0.8704435488632091]\n",
      "Validation- [np.float64(0.5606060606060606), 0.606954875906367, 0.5204830483754755]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6268958362515759, 0.5703336686733054]\n",
      "\n",
      "Epoch 96/200\n",
      "Training Loss: 0.4017\n",
      "Train     - [np.float64(0.8658008658008658), 0.9781788437373544, 0.8593239656934597]\n",
      "Validation- [np.float64(0.5909090909090909), 0.6106533760985149, 0.5574590827397955]\n",
      "Test      - [np.float64(0.601010101010101), 0.6123284073399635, 0.5661041100175986]\n",
      "\n",
      "Epoch 97/200\n",
      "Training Loss: 0.3985\n",
      "Train     - [np.float64(0.8939393939393939), 0.9789959174600131, 0.9032089435167334]\n",
      "Validation- [np.float64(0.5883838383838383), 0.598119908613707, 0.5884664553970318]\n",
      "Test      - [np.float64(0.6338383838383839), 0.6186169281411962, 0.6407373861981897]\n",
      "\n",
      "Epoch 98/200\n",
      "Training Loss: 0.4081\n",
      "Train     - [np.float64(0.8831168831168831), 0.9796438060400827, 0.8802438382739131]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5956243903005528, 0.54976854351499]\n",
      "Test      - [np.float64(0.6111111111111112), 0.6255112760890882, 0.5794352460241775]\n",
      "\n",
      "Epoch 99/200\n",
      "Training Loss: 0.4129\n",
      "Train     - [np.float64(0.8869047619047619), 0.9803203136647285, 0.8845382381435508]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5904790548444447, 0.5483947206519109]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6270718062753886, 0.5934245071249461]\n",
      "\n",
      "Epoch 100/200\n",
      "Training Loss: 0.4025\n",
      "Train     - [np.float64(0.887987012987013), 0.9780239015611356, 0.8975387967391573]\n",
      "Validation- [np.float64(0.5580808080808081), 0.5832608468434988, 0.5593145331170615]\n",
      "Test      - [np.float64(0.6464646464646465), 0.6294193864686931, 0.6525171132294582]\n",
      "\n",
      "Epoch 101/200\n",
      "Training Loss: 0.4003\n",
      "Train     - [np.float64(0.8760822510822511), 0.9792845704747299, 0.8819678819865955]\n",
      "Validation- [np.float64(0.5732323232323232), 0.5779255279838849, 0.5563918720717634]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6176307080823644, 0.602262853104465]\n",
      "\n",
      "Epoch 102/200\n",
      "Training Loss: 0.4102\n",
      "Train     - [np.float64(0.9101731601731602), 0.9800449718335357, 0.9069361758040622]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5855891069143613, 0.5374944454752918]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6090804034178456, 0.5659868464774498]\n",
      "\n",
      "Epoch 103/200\n",
      "Training Loss: 0.3920\n",
      "Train     - [np.float64(0.8804112554112554), 0.981808088008221, 0.8731022614125171]\n",
      "Validation- [np.float64(0.5883838383838383), 0.5870582186429727, 0.5523285916538146]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6113785194004762, 0.5614920031171189]\n",
      "\n",
      "Epoch 104/200\n",
      "Training Loss: 0.4090\n",
      "Train     - [np.float64(0.8917748917748918), 0.9799410946851106, 0.9022535356189241]\n",
      "Validation- [np.float64(0.5782828282828283), 0.578862591164584, 0.5703983125968343]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6015430207311949, 0.6126960019426169]\n",
      "\n",
      "Epoch 105/200\n",
      "Training Loss: 0.4049\n",
      "Train     - [np.float64(0.9112554112554112), 0.9814038772815368, 0.9161366117762165]\n",
      "Validation- [np.float64(0.5833333333333334), 0.5906640497979283, 0.5778437209261786]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6097864721949853, 0.6187267213929317]\n",
      "\n",
      "Epoch 106/200\n",
      "Training Loss: 0.3989\n",
      "Train     - [np.float64(0.9090909090909091), 0.9833041652024366, 0.9155655770558521]\n",
      "Validation- [np.float64(0.5934343434343434), 0.5890488477472265, 0.5934810312404373]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6109175829948172, 0.5995328136700011]\n",
      "\n",
      "Epoch 107/200\n",
      "Training Loss: 0.3880\n",
      "Train     - [np.float64(0.9015151515151515), 0.9827629990362704, 0.9055401393721622]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5882728402386854, 0.5573726857809894]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6050287155063734, 0.5795393304057586]\n",
      "\n",
      "Epoch 108/200\n",
      "Training Loss: 0.3899\n",
      "Train     - [np.float64(0.9258658008658008), 0.9823126816976551, 0.9294901759255112]\n",
      "Validation- [np.float64(0.5404040404040404), 0.5711333969602655, 0.5522418539943091]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6034051512816921, 0.600935160438351]\n",
      "\n",
      "Epoch 109/200\n",
      "Training Loss: 0.3818\n",
      "Train     - [np.float64(0.8831168831168831), 0.9810591675452047, 0.8819843766789927]\n",
      "Validation- [np.float64(0.5883838383838383), 0.5767764160863524, 0.5684726912556308]\n",
      "Test      - [np.float64(0.601010101010101), 0.6178557045804735, 0.5650720361110736]\n",
      "\n",
      "Epoch 110/200\n",
      "Training Loss: 0.3726\n",
      "Train     - [np.float64(0.887987012987013), 0.9824772911209081, 0.898438971666261]\n",
      "Validation- [np.float64(0.5934343434343434), 0.5832248191488913, 0.5901022952212491]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6159152192183779, 0.5988459821877175]\n",
      "\n",
      "Epoch 111/200\n",
      "Training Loss: 0.4370\n",
      "Train     - [np.float64(0.8923160173160173), 0.9813317140278577, 0.8873295288003284]\n",
      "Validation- [np.float64(0.5681818181818182), 0.5904040169427822, 0.5319002365676988]\n",
      "Test      - [np.float64(0.5883838383838383), 0.6160636118503993, 0.5527081263511904]\n",
      "\n",
      "Epoch 112/200\n",
      "Training Loss: 0.4059\n",
      "Train     - [np.float64(0.8722943722943723), 0.9800303295316593, 0.877302565702219]\n",
      "Validation- [np.float64(0.5934343434343434), 0.5884795573845951, 0.5797844529741035]\n",
      "Test      - [np.float64(0.6388888888888888), 0.6173575605827146, 0.6274966396192774]\n",
      "\n",
      "Epoch 113/200\n",
      "Training Loss: 0.4123\n",
      "Train     - [np.float64(0.908008658008658), 0.9832994563712649, 0.9129610111526882]\n",
      "Validation- [np.float64(0.5681818181818182), 0.5800776614568597, 0.5741691143303647]\n",
      "Test      - [np.float64(0.6287878787878788), 0.6203052773497688, 0.6388550860987717]\n",
      "\n",
      "Epoch 114/200\n",
      "Training Loss: 0.4048\n",
      "Train     - [np.float64(0.9053030303030303), 0.982892134155432, 0.9124302053108176]\n",
      "Validation- [np.float64(0.5707070707070707), 0.5797645240438013, 0.5796342880202217]\n",
      "Test      - [np.float64(0.6161616161616161), 0.6190551022552178, 0.6251598005089674]\n",
      "\n",
      "Epoch 115/200\n",
      "Training Loss: 0.4126\n",
      "Train     - [np.float64(0.8928571428571429), 0.9838664961504058, 0.8950361369726797]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5847199816720652, 0.5657589109278642]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6216119204370361, 0.6155437054392059]\n",
      "\n",
      "Epoch 116/200\n",
      "Training Loss: 0.3995\n",
      "Train     - [np.float64(0.9264069264069265), 0.9830271261953443, 0.9313012306358972]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5924158139672377, 0.6043719368450086]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6109302773497688, 0.6398737127614915]\n",
      "\n",
      "Epoch 117/200\n",
      "Training Loss: 0.4093\n",
      "Train     - [np.float64(0.8933982683982684), 0.9795233897461291, 0.8943228256893402]\n",
      "Validation- [np.float64(0.5707070707070707), 0.5794630263474622, 0.5618499633816924]\n",
      "Test      - [np.float64(0.6136363636363636), 0.617889410281552, 0.6074416196340089]\n",
      "\n",
      "Epoch 118/200\n",
      "Training Loss: 0.4148\n",
      "Train     - [np.float64(0.9161255411255411), 0.9839428191489361, 0.9229849717810695]\n",
      "Validation- [np.float64(0.5631313131313131), 0.5869116796243196, 0.5814931802689559]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6124921207452023, 0.6379032143976215]\n",
      "\n",
      "Epoch 119/200\n",
      "Training Loss: 0.4024\n",
      "Train     - [np.float64(0.9020562770562771), 0.981151630353758, 0.910185446081556]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5996157309847676, 0.5859987211297298]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6231435600224121, 0.6335815485711017]\n",
      "\n",
      "Epoch 120/200\n",
      "Training Loss: 0.4064\n",
      "Train     - [np.float64(0.8582251082251082), 0.9840306396822354, 0.8575324289405193]\n",
      "Validation- [np.float64(0.5883838383838383), 0.5992257872777105, 0.5631615228585045]\n",
      "Test      - [np.float64(0.6161616161616161), 0.6247229128729513, 0.5797491060472735]\n",
      "\n",
      "Epoch 121/200\n",
      "Training Loss: 0.3914\n",
      "Train     - [np.float64(0.8933982683982684), 0.983948975571316, 0.89052085063082]\n",
      "Validation- [np.float64(0.5707070707070707), 0.5825349349865074, 0.5359486815881408]\n",
      "Test      - [np.float64(0.6262626262626263), 0.6186515093150302, 0.6081228548019164]\n",
      "\n",
      "Epoch 122/200\n",
      "Training Loss: 0.4021\n",
      "Train     - [np.float64(0.8722943722943723), 0.9817059579526335, 0.870168374986933]\n",
      "Validation- [np.float64(0.5959595959595959), 0.5882662945476506, 0.5683453925046136]\n",
      "Test      - [np.float64(0.5934343434343434), 0.6197042653032637, 0.5537022546717293]\n",
      "\n",
      "Epoch 123/200\n",
      "Training Loss: 0.3873\n",
      "Train     - [np.float64(0.908008658008658), 0.9812993345739356, 0.9097212481264224]\n",
      "Validation- [np.float64(0.5656565656565656), 0.587534285696187, 0.5526891678738001]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6291352955596022, 0.6208653861786176]\n",
      "\n",
      "Epoch 124/200\n",
      "Training Loss: 0.3989\n",
      "Train     - [np.float64(0.8501082251082251), 0.982060218463144, 0.8602232586631272]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5977844947359976, 0.5769473712148366]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6201573224541252, 0.5792623889652769]\n",
      "\n",
      "Epoch 125/200\n",
      "Training Loss: 0.3912\n",
      "Train     - [np.float64(0.8663419913419913), 0.9808752236278833, 0.8754006681010333]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5899031132151167, 0.5784332745251833]\n",
      "Test      - [np.float64(0.6388888888888888), 0.6265710358593641, 0.6308008777753628]\n",
      "\n",
      "Epoch 126/200\n",
      "Training Loss: 0.4189\n",
      "Train     - [np.float64(0.8939393939393939), 0.9806862214608225, 0.8920425409826123]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5936060475850623, 0.5406955780052881]\n",
      "Test      - [np.float64(0.5959595959595959), 0.6191535929401877, 0.5638935701312154]\n",
      "\n",
      "Epoch 127/200\n",
      "Training Loss: 0.3927\n",
      "Train     - [np.float64(0.8701298701298701), 0.9811613142397717, 0.8719475701093417]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5971079131070073, 0.5752098604475788]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6190822419106317, 0.6033946379902435]\n",
      "\n",
      "Epoch 128/200\n",
      "Training Loss: 0.3868\n",
      "Train     - [np.float64(0.8912337662337663), 0.9810436100994612, 0.8873599997117755]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5963465278276329, 0.554243861399119]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6260046049866927, 0.5945468503125921]\n",
      "\n",
      "Epoch 129/200\n",
      "Training Loss: 0.4118\n",
      "Train     - [np.float64(0.9004329004329005), 0.9815413318904012, 0.9047581041200656]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5948640079899661, 0.5546270522169583]\n",
      "Test      - [np.float64(0.6287878787878788), 0.6255449817901668, 0.6221084656421699]\n",
      "\n",
      "Epoch 130/200\n",
      "Training Loss: 0.4018\n",
      "Train     - [np.float64(0.8982683982683982), 0.9827762103859178, 0.9041469825982813]\n",
      "Validation- [np.float64(0.5505050505050505), 0.589196600885991, 0.552497215038544]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6209290516879116, 0.6140537783697818]\n",
      "\n",
      "Epoch 131/200\n",
      "Training Loss: 0.3931\n",
      "Train     - [np.float64(0.8863636363636364), 0.984630042276319, 0.8959709765511508]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5843200610649628, 0.5839010898315011]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6240566781061774, 0.608931612308048]\n",
      "\n",
      "Epoch 132/200\n",
      "Training Loss: 0.4318\n",
      "Train     - [np.float64(0.8685064935064936), 0.9842220045843716, 0.8885355828869104]\n",
      "Validation- [np.float64(0.5681818181818182), 0.5916359265531235, 0.5741070794472224]\n",
      "Test      - [np.float64(0.6313131313131313), 0.6252394417985713, 0.6451348529767046]\n",
      "\n",
      "Epoch 133/200\n",
      "Training Loss: 0.4041\n",
      "Train     - [np.float64(0.8966450216450217), 0.9852620905479949, 0.8994602334245372]\n",
      "Validation- [np.float64(0.5631313131313131), 0.5884783432644839, 0.5539243025405808]\n",
      "Test      - [np.float64(0.6035353535353535), 0.6179668896203949, 0.5932471056077154]\n",
      "\n",
      "Epoch 134/200\n",
      "Training Loss: 0.3935\n",
      "Train     - [np.float64(0.8917748917748918), 0.986018565107661, 0.8987588526378966]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5977565699734372, 0.5963695400203328]\n",
      "Test      - [np.float64(0.6085858585858586), 0.608007949292618, 0.6086301720591601]\n",
      "\n",
      "Epoch 135/200\n",
      "Training Loss: 0.4155\n",
      "Train     - [np.float64(0.8977272727272727), 0.9850745026941835, 0.8956019170194612]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5846237758502009, 0.5416411284961595]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6265894207872251, 0.5789456630528926]\n",
      "\n",
      "Epoch 136/200\n",
      "Training Loss: 0.3847\n",
      "Train     - [np.float64(0.8890692640692641), 0.9829984405948502, 0.8817304177188031]\n",
      "Validation- [np.float64(0.5681818181818182), 0.5839280058446688, 0.5452134893846494]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6216237393192323, 0.5898404302991794]\n",
      "\n",
      "Epoch 137/200\n",
      "Training Loss: 0.3806\n",
      "Train     - [np.float64(0.8852813852813853), 0.9847222388612016, 0.8922119435959537]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5858130064992377, 0.5898890125595734]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6230708957837233, 0.6171906260036469]\n",
      "\n",
      "Epoch 138/200\n",
      "Training Loss: 0.3884\n",
      "Train     - [np.float64(0.913961038961039), 0.9833210038495943, 0.9133337082301839]\n",
      "Validation- [np.float64(0.5833333333333334), 0.597544415680942, 0.576679791441931]\n",
      "Test      - [np.float64(0.6313131313131313), 0.6356247373581734, 0.6338867297496845]\n",
      "\n",
      "Epoch 139/200\n",
      "Training Loss: 0.3881\n",
      "Train     - [np.float64(0.8998917748917749), 0.9848636535471642, 0.8944310861517962]\n",
      "Validation- [np.float64(0.5833333333333334), 0.5950778250991355, 0.5574658397153746]\n",
      "Test      - [np.float64(0.6111111111111112), 0.621598350609329, 0.5852324145462661]\n",
      "\n",
      "Epoch 140/200\n",
      "Training Loss: 0.3898\n",
      "Train     - [np.float64(0.9199134199134199), 0.9856730733392968, 0.9245473559128135]\n",
      "Validation- [np.float64(0.5707070707070707), 0.5867494098320503, 0.5812742617078872]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6125017509455105, 0.6352384993812137]\n",
      "\n",
      "Epoch 141/200\n",
      "Training Loss: 0.3853\n",
      "Train     - [np.float64(0.8739177489177489), 0.9835523355802611, 0.8623312818875196]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5904901402889395, 0.5185757180706236]\n",
      "Test      - [np.float64(0.6161616161616161), 0.6204790586916935, 0.5604013058631642]\n",
      "\n",
      "Epoch 142/200\n",
      "Training Loss: 0.3817\n",
      "Train     - [np.float64(0.9015151515151515), 0.9847041190126296, 0.9007977642461442]\n",
      "Validation- [np.float64(0.5883838383838383), 0.6006653114270873, 0.5737241153086944]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6147560932903768, 0.5952958693924404]\n",
      "\n",
      "Epoch 143/200\n",
      "Training Loss: 0.3942\n",
      "Train     - [np.float64(0.8977272727272727), 0.9822195533299256, 0.8973110470023075]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5883412268736513, 0.5448447155043094]\n",
      "Test      - [np.float64(0.601010101010101), 0.6213777314749964, 0.5621846359045981]\n",
      "\n",
      "Epoch 144/200\n",
      "Training Loss: 0.3712\n",
      "Train     - [np.float64(0.8793290043290043), 0.9841807731934062, 0.8870726031211976]\n",
      "Validation- [np.float64(0.5782828282828283), 0.5889633842489558, 0.5726125917268592]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6127004832609609, 0.6115662966349139]\n",
      "\n",
      "Epoch 145/200\n",
      "Training Loss: 0.4047\n",
      "Train     - [np.float64(0.8706709956709957), 0.9830433325612846, 0.8784680244516696]\n",
      "Validation- [np.float64(0.5883838383838383), 0.596759645392467, 0.5801512632174237]\n",
      "Test      - [np.float64(0.6313131313131313), 0.6308560372601204, 0.6231204580224455]\n",
      "\n",
      "Epoch 146/200\n",
      "Training Loss: 0.4117\n",
      "Train     - [np.float64(0.8966450216450217), 0.9842990097810577, 0.8976852676834307]\n",
      "Validation- [np.float64(0.5934343434343434), 0.5856692388417083, 0.5820035901331212]\n",
      "Test      - [np.float64(0.6287878787878788), 0.6087595426530327, 0.6069418937088997]\n",
      "\n",
      "Epoch 147/200\n",
      "Training Loss: 0.3933\n",
      "Train     - [np.float64(0.9242424242424242), 0.9865518943145273, 0.9278086588918579]\n",
      "Validation- [np.float64(0.5580808080808081), 0.5923541841746307, 0.5626310426058095]\n",
      "Test      - [np.float64(0.6111111111111112), 0.6100469253396834, 0.6156178690967596]\n",
      "\n",
      "Epoch 148/200\n",
      "Training Loss: 0.3604\n",
      "Train     - [np.float64(0.8901515151515151), 0.9850885293538219, 0.8885477806654082]\n",
      "Validation- [np.float64(0.6060606060606061), 0.6036518621435238, 0.589934424925625]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6102075745902787, 0.6010914118157574]\n",
      "\n",
      "Epoch 149/200\n",
      "Training Loss: 0.3967\n",
      "Train     - [np.float64(0.8809523809523809), 0.9842808067375887, 0.8801704205515067]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5900542975628914, 0.5686896286616235]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6057982560582715, 0.6059079901737012]\n",
      "\n",
      "Epoch 150/200\n",
      "Training Loss: 0.3850\n",
      "Train     - [np.float64(0.8690476190476191), 0.9848486618267203, 0.8651083103719985]\n",
      "Validation- [np.float64(0.6111111111111112), 0.5850626010886963, 0.5841504737823596]\n",
      "Test      - [np.float64(0.6363636363636364), 0.6182824975486763, 0.5998651035003904]\n",
      "\n",
      "Epoch 151/200\n",
      "Training Loss: 0.3817\n",
      "Train     - [np.float64(0.8923160173160173), 0.9830533159489276, 0.8984122225611124]\n",
      "Validation- [np.float64(0.5833333333333334), 0.5870844805888589, 0.5726108737743847]\n",
      "Test      - [np.float64(0.6313131313131313), 0.6139979338842975, 0.621439605929039]\n",
      "\n",
      "Epoch 152/200\n",
      "Training Loss: 0.3843\n",
      "Train     - [np.float64(0.8955627705627706), 0.9853732555693993, 0.8937339718608346]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5978655240564703, 0.5310160792909466]\n",
      "Test      - [np.float64(0.6262626262626263), 0.6221630305364897, 0.6021475782956902]\n",
      "\n",
      "Epoch 153/200\n",
      "Training Loss: 0.3798\n",
      "Train     - [np.float64(0.8392857142857143), 0.9835117198315337, 0.8468298811647476]\n",
      "Validation- [np.float64(0.5959595959595959), 0.5857348541155505, 0.5862375374902481]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6156074905448943, 0.6171816303618463]\n",
      "\n",
      "Epoch 154/200\n",
      "Training Loss: 0.3671\n",
      "Train     - [np.float64(0.8787878787878788), 0.9858832402881605, 0.8821885561746919]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5850659267220446, 0.5739896990070007]\n",
      "Test      - [np.float64(0.6287878787878788), 0.6334207346967363, 0.6181628133611181]\n",
      "\n",
      "Epoch 155/200\n",
      "Training Loss: 0.4077\n",
      "Train     - [np.float64(0.928030303030303), 0.985373039262667, 0.9312562199398786]\n",
      "Validation- [np.float64(0.5580808080808081), 0.5920603407137759, 0.5614591293194475]\n",
      "Test      - [np.float64(0.6111111111111112), 0.6191781061773358, 0.6291767436718079]\n",
      "\n",
      "Epoch 156/200\n",
      "Training Loss: 0.3753\n",
      "Train     - [np.float64(0.895021645021645), 0.9839448990213618, 0.8982701169990044]\n",
      "Validation- [np.float64(0.5681818181818182), 0.591316692145593, 0.5617930499529008]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6126365737498249, 0.6105495375270872]\n",
      "\n",
      "Epoch 157/200\n",
      "Training Loss: 0.3918\n",
      "Train     - [np.float64(0.8858225108225108), 0.9852693451430152, 0.8791324053422459]\n",
      "Validation- [np.float64(0.5656565656565656), 0.5835197711541953, 0.519839258349444]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6228043143297379, 0.5813566774437435]\n",
      "\n",
      "Epoch 158/200\n",
      "Training Loss: 0.3863\n",
      "Train     - [np.float64(0.8906926406926406), 0.9828472588279769, 0.8891643157763892]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5663414232443824, 0.5315497577901405]\n",
      "Test      - [np.float64(0.6186868686868687), 0.6234591679506933, 0.5993059297711315]\n",
      "\n",
      "Epoch 159/200\n",
      "Training Loss: 0.4066\n",
      "Train     - [np.float64(0.8928571428571429), 0.9844804079345303, 0.8968195107376122]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5717428060744013, 0.5719721012386]\n",
      "Test      - [np.float64(0.6338383838383839), 0.6226524198066956, 0.6332302050589471]\n",
      "\n",
      "Epoch 160/200\n",
      "Training Loss: 0.3931\n",
      "Train     - [np.float64(0.91504329004329), 0.9881040947862756, 0.9141820585056918]\n",
      "Validation- [np.float64(0.5707070707070707), 0.5708770064654536, 0.5503707711199316]\n",
      "Test      - [np.float64(0.6262626262626263), 0.6216508789746463, 0.6126283730444662]\n",
      "\n",
      "Epoch 161/200\n",
      "Training Loss: 0.3940\n",
      "Train     - [np.float64(0.9112554112554112), 0.985748031941516, 0.9093969934002216]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5717688568689637, 0.5234120734717543]\n",
      "Test      - [np.float64(0.6388888888888888), 0.6267360624737358, 0.6231381357113157]\n",
      "\n",
      "Epoch 162/200\n",
      "Training Loss: 0.3884\n",
      "Train     - [np.float64(0.8852813852813853), 0.9866568197186548, 0.8909777730206266]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5843962075110749, 0.5713819790118279]\n",
      "Test      - [np.float64(0.6237373737373737), 0.6237550777419807, 0.6236562754216811]\n",
      "\n",
      "Epoch 163/200\n",
      "Training Loss: 0.3762\n",
      "Train     - [np.float64(0.8928571428571429), 0.9864787160831043, 0.8939157313288123]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5922917889584751, 0.5640022113613169]\n",
      "Test      - [np.float64(0.6262626262626263), 0.6243709728253257, 0.6172849075715632]\n",
      "\n",
      "Epoch 164/200\n",
      "Training Loss: 0.3819\n",
      "Train     - [np.float64(0.9161255411255411), 0.9844824378900177, 0.9155576116361761]\n",
      "Validation- [np.float64(0.5808080808080808), 0.5863001589969468, 0.5560993290459568]\n",
      "Test      - [np.float64(0.6136363636363636), 0.6250766038660877, 0.6017098213791184]\n",
      "\n",
      "Epoch 165/200\n",
      "Training Loss: 0.3832\n",
      "Train     - [np.float64(0.8939393939393939), 0.9856314259738461, 0.9009745537532631]\n",
      "Validation- [np.float64(0.5833333333333334), 0.5776000910062205, 0.566062561639319]\n",
      "Test      - [np.float64(0.5984848484848485), 0.6105595146379044, 0.5860083493197751]\n",
      "\n",
      "Epoch 166/200\n",
      "Training Loss: 0.3975\n",
      "Train     - [np.float64(0.8988095238095238), 0.98428049059698, 0.9002613546982372]\n",
      "Validation- [np.float64(0.5858585858585859), 0.5840395201375017, 0.5681470948897717]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6204523567726572, 0.5875841295339423]\n",
      "\n",
      "Epoch 167/200\n",
      "Training Loss: 0.3824\n",
      "Train     - [np.float64(0.9117965367965368), 0.9855287468319383, 0.9118975557526507]\n",
      "Validation- [np.float64(0.5606060606060606), 0.5837401339543997, 0.5421989577758396]\n",
      "Test      - [np.float64(0.6060606060606061), 0.6295931678106177, 0.5828779549400432]\n",
      "\n",
      "Epoch 168/200\n",
      "Training Loss: 0.3859\n",
      "Train     - [np.float64(0.8300865800865801), 0.9838457639820672, 0.8251209565904359]\n",
      "Validation- [np.float64(0.5909090909090909), 0.5940170536366592, 0.5553179123021651]\n",
      "Test      - [np.float64(0.6085858585858586), 0.6341268034738758, 0.567520795709797]\n",
      "\n",
      "Epoch 169/200\n",
      "Training Loss: 0.3650\n",
      "Train     - [np.float64(0.8906926406926406), 0.9852793118916789, 0.8885769941334256]\n",
      "Validation- [np.float64(0.5833333333333334), 0.5875354998162984, 0.5582052365768311]\n",
      "Test      - [np.float64(0.6161616161616161), 0.6209723875892983, 0.5903268826285881]\n",
      "\n",
      "Epoch 170/200\n",
      "Training Loss: 0.3743\n",
      "Train     - [np.float64(0.9188311688311688), 0.9842317716652823, 0.9201506088657719]\n",
      "Validation- [np.float64(0.5429292929292929), 0.5743371959948818, 0.5259466902569814]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6262633071858803, 0.6118638074674217]\n",
      "\n",
      "Epoch 171/200\n",
      "Training Loss: 0.3873\n",
      "Train     - [np.float64(0.9004329004329005), 0.9836009380391029, 0.9016565766608723]\n",
      "Validation- [np.float64(0.5631313131313131), 0.5812753117649294, 0.5368964165431509]\n",
      "Test      - [np.float64(0.6313131313131313), 0.6381123931923238, 0.6163069475498361]\n",
      "\n",
      "Epoch 172/200\n",
      "Training Loss: 0.3690\n",
      "Train     - [np.float64(0.9172077922077922), 0.9845665479309096, 0.9174046923127742]\n",
      "Validation- [np.float64(0.5631313131313131), 0.5881807254737179, 0.5442822438027723]\n",
      "Test      - [np.float64(0.6287878787878788), 0.6475298536209553, 0.6194494708925011]\n",
      "\n",
      "Epoch 173/200\n",
      "Training Loss: 0.3727\n",
      "Train     - [np.float64(0.9145021645021645), 0.9838993414957511, 0.9223284700432637]\n",
      "Validation- [np.float64(0.5580808080808081), 0.5775629547671635, 0.5720342627638639]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6330911192043703, 0.6429380021522472]\n",
      "\n",
      "Epoch 174/200\n",
      "Training Loss: 0.3803\n",
      "Train     - [np.float64(0.9047619047619048), 0.9842657983781653, 0.9103045589229176]\n",
      "Validation- [np.float64(0.5757575757575758), 0.5790986847384046, 0.5609110927350275]\n",
      "Test      - [np.float64(0.6212121212121212), 0.6399666444880234, 0.6101221348151792]\n",
      "\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\Desktop\\Facultate\\DEAP_Analysis\\.venv\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Facultate\\DEAP_Analysis\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Facultate\\DEAP_Analysis\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    net.train()  # training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Shuffle indices\n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i + batch_size]\n",
    "        batch_x = X_train[batch_idx]\n",
    "        batch_y = y_train[batch_idx]\n",
    "\n",
    "        inputs = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "        labels = torch.tensor(batch_y, dtype=torch.float32).to(device)  # NO unsqueeze\n",
    "        # unsqueeze(1) makes shape [B,1] instead of [B]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # End of epoch summary\n",
    "    avg_loss = running_loss / (len(X_train) / batch_size)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    net.eval()\n",
    "    params = [\"acc\", \"auc\", \"fmeasure\"]\n",
    "\n",
    "    print(\"Train     -\", evaluate(net, X_train, y_train, params, device=device))\n",
    "    print(\"Validation-\", evaluate(net, X_val, y_val, params, device=device))\n",
    "    print(\"Test      -\", evaluate(net, X_test, y_test, params, device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
