{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-10-06T17:52:34.038753Z",
     "start_time": "2025-10-06T17:52:34.032349Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Written by, \n",
    "Sriram Ravindran, sriram@ucsd.edu\n",
    "\n",
    "Original paper - https://arxiv.org/abs/1611.08024\n",
    "\n",
    "Please reach out to me if you spot an error.\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWritten by, \\nSriram Ravindran, sriram@ucsd.edu\\n\\nOriginal paper - https://arxiv.org/abs/1611.08024\\n\\nPlease reach out to me if you spot an error.\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T17:52:34.168578Z",
     "start_time": "2025-10-06T17:52:34.152058Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Compiled with CUDA:\", torch.version.cuda)\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Compiled with CUDA: 12.1\n",
      "GPU name: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here's the description from the paper</p>\n",
    "<img src=\"EEGNet.png\" style=\"width: 700px; float:left;\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T17:52:34.284788Z",
     "start_time": "2025-10-06T17:52:34.255614Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------\n",
    "# Squeeze-and-Excitation Block\n",
    "# ---------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# ---------------------------\n",
    "# EEGNet with SE Attention\n",
    "# ---------------------------\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, T=120, C=32, dropout=0.25):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = T\n",
    "        self.C = C\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Layer 1: temporal conv across channels\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, C))  # kernel spans all 32 channels\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Layer 2: spatial conv (depthwise) with more filters\n",
    "        self.padding1 = nn.ZeroPad2d((8, 8, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 16, (1, 16))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.pooling2 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # SE block after Layer 2\n",
    "        self.se2 = SEBlock(16)\n",
    "\n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(16, 16, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(16)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "\n",
    "        # Dynamically infer flatten size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, T, C)\n",
    "            out = self._forward_features(dummy)\n",
    "            flatten_dim = out.shape[1]\n",
    "        print(f\"[EEGNet] Flattened feature dimension: {flatten_dim}\")\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(flatten_dim, 2)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Apply SE attention\n",
    "        x = self.se2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Usage Example\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = EEGNet(T=120, C=32, dropout=0.25).to(device)\n",
    "x_dummy = torch.rand(1, 1, 120, 32).to(device)\n",
    "output = net(x_dummy)\n",
    "print(\"Output:\", output)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EEGNet] Flattened feature dimension: 448\n",
      "Output: tensor([[ 0.0473, -0.2404]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate function returns values of different criteria like accuracy, precision etc.\n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T17:52:34.402041Z",
     "start_time": "2025-10-06T17:52:34.389074Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "\n",
    "def evaluate(model, X, Y, params=[\"acc\"], batch_size=100, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate a trained multi-label EEGNet model on given data.\n",
    "\n",
    "    Args:\n",
    "        model: torch.nn.Module\n",
    "        X: numpy array, shape [samples, 1, timepoints, channels]\n",
    "        Y: numpy array, shape [samples, n_labels] (e.g., valence+arousal)\n",
    "        params: list of metrics to compute ['acc', 'auc', 'precision', 'recall', 'fmeasure']\n",
    "        batch_size: batch size for evaluation\n",
    "        device: torch.device (default: cuda if available)\n",
    "\n",
    "    Returns:\n",
    "        results: list of computed metrics (average across labels)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    preds = []\n",
    "\n",
    "    # Iterate over batches\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_x = X[i:i + batch_size]\n",
    "        inputs = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():  # disable gradient computation\n",
    "            output = model(inputs)        # raw logits\n",
    "            output = torch.sigmoid(output)  # convert logits â†’ probabilities\n",
    "\n",
    "        preds.append(output.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    predicted = np.vstack(preds)  # shape: [samples, n_labels]\n",
    "\n",
    "    results = []\n",
    "    for param in params:\n",
    "        if param == \"acc\":\n",
    "            results.append(np.mean(np.round(predicted) == Y))  # average accuracy over labels\n",
    "        elif param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted, average='macro'))\n",
    "        elif param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted), average='macro'))\n",
    "        elif param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted), average='macro'))\n",
    "        elif param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted), average='macro')\n",
    "            recall = recall_score(Y, np.round(predicted), average='macro')\n",
    "            results.append(2 * precision * recall / (precision + recall + 1e-8))  # avoid div0\n",
    "\n",
    "    model.train()  # switch back to training mode\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T17:52:36.840786Z",
     "start_time": "2025-10-06T17:52:34.494194Z"
    }
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Path to your DEAP folder\n",
    "data_dir = r\"G:\\DEAP\\data_preprocessed_python\"\n",
    "\n",
    "# Initialize lists\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop through subjects s01â€“s32\n",
    "for i in range(1, 33):\n",
    "    filename = f\"s{i:02d}.dat\"\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    print(f\"Loading {filename}...\")\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        subject_data = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "    data = subject_data[\"data\"]      # shape: (40, 40, 8064)\n",
    "    labels = subject_data[\"labels\"]  # shape: (40, 4)\n",
    "\n",
    "    all_data.append(data)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Stack into single numpy arrays\n",
    "data_all = np.vstack(all_data)       # shape: (1280, 40, 8064)\n",
    "labels_all = np.vstack(all_labels)   # shape: (1280, 4)\n",
    "\n",
    "print(\"âœ… All subjects loaded!\")\n",
    "print(\"Data shape:\", data_all.shape)\n",
    "print(\"Labels shape:\", labels_all.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading s01.dat...\n",
      "Loading s02.dat...\n",
      "Loading s03.dat...\n",
      "Loading s04.dat...\n",
      "Loading s05.dat...\n",
      "Loading s06.dat...\n",
      "Loading s07.dat...\n",
      "Loading s08.dat...\n",
      "Loading s09.dat...\n",
      "Loading s10.dat...\n",
      "Loading s11.dat...\n",
      "Loading s12.dat...\n",
      "Loading s13.dat...\n",
      "Loading s14.dat...\n",
      "Loading s15.dat...\n",
      "Loading s16.dat...\n",
      "Loading s17.dat...\n",
      "Loading s18.dat...\n",
      "Loading s19.dat...\n",
      "Loading s20.dat...\n",
      "Loading s21.dat...\n",
      "Loading s22.dat...\n",
      "Loading s23.dat...\n",
      "Loading s24.dat...\n",
      "Loading s25.dat...\n",
      "Loading s26.dat...\n",
      "Loading s27.dat...\n",
      "Loading s28.dat...\n",
      "Loading s29.dat...\n",
      "Loading s30.dat...\n",
      "Loading s31.dat...\n",
      "Loading s32.dat...\n",
      "âœ… All subjects loaded!\n",
      "Data shape: (1280, 40, 8064)\n",
      "Labels shape: (1280, 4)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T17:52:38.301445Z",
     "start_time": "2025-10-06T17:52:36.925934Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "n_trials, n_channels, n_samples = data_all.shape\n",
    "data_all = data_all[:, :32, :]  # keep first 32 channels\n",
    "segment_len = 120\n",
    "step = segment_len // 2  # 50% overlap\n",
    "n_segments = (n_samples - segment_len) // step + 1  # total segments per trial\n",
    "\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "print(f\"Segmenting {n_trials} trials into {n_segments} overlapping segments each...\")\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    trial_data = data_all[trial]\n",
    "    trial_label = labels_all[trial]\n",
    "\n",
    "    for seg in range(n_segments):\n",
    "        start = seg * step\n",
    "        end = start + segment_len\n",
    "        segment = trial_data[:, start:end].T  # shape: (120, 32)\n",
    "\n",
    "        X_list.append(segment[np.newaxis, :, :])\n",
    "\n",
    "        valence = int(trial_label[0] > 5.0)\n",
    "        arousal = int(trial_label[1] > 5.0)\n",
    "        Y_list.append([valence, arousal])\n",
    "\n",
    "X = np.array(X_list, dtype=np.float32)\n",
    "Y = np.array(Y_list, dtype=np.float32)\n",
    "\n",
    "print(\"âœ… Done segmenting with 50% overlap!\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting 1280 trials into 133 overlapping segments each...\n",
      "âœ… Done segmenting with 50% overlap!\n",
      "X shape: (170240, 1, 120, 32)\n",
      "Y shape: (170240, 2)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-10-06T17:52:39.171516Z",
     "start_time": "2025-10-06T17:52:38.401804Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 70% training, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (119168, 1, 120, 32) y_train: (119168, 2)\n",
      "X_val: (25536, 1, 120, 32) y_val: (25536, 2)\n",
      "X_test: (25536, 1, 120, 32) y_test: (25536, 2)\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-10-06T18:59:00.467243Z",
     "start_time": "2025-10-06T17:52:39.295946Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Hyperparameters\n",
    "# ------------------------------\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 1.0  # optional gradient clipping\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume `net`, `criterion`, `X_train`, `y_train`, `X_val`, `y_val`, `X_test`, `y_test` exist\n",
    "net = net.to(device)\n",
    "optimizer = Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)  # reduce LR every 30 epochs\n",
    "scaler = GradScaler()  # AMP scaler\n",
    "\n",
    "params = [\"acc\", \"auc\", \"fmeasure\"]\n",
    "\n",
    "# ------------------------------\n",
    "# Training loop\n",
    "# ------------------------------\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Shuffle training indices\n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_idx = indices[i:i + batch_size]\n",
    "        batch_x = torch.tensor(X_train[batch_idx], dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(y_train[batch_idx], dtype=torch.float32).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AMP forward + backward\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = net(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Optional gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()  # update learning rate\n",
    "    avg_loss = running_loss / (len(X_train) / batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Evaluation\n",
    "    # ------------------------------\n",
    "    net.eval()\n",
    "    train_metrics = evaluate(net, X_train, y_train, params, device=device)\n",
    "    val_metrics = evaluate(net, X_val, y_val, params, device=device)\n",
    "    test_metrics = evaluate(net, X_test, y_test, params, device=device)\n",
    "\n",
    "    print(f\"Train     - {train_metrics}\")\n",
    "    print(f\"Validation- {val_metrics}\")\n",
    "    print(f\"Test      - {test_metrics}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Training Loss: 0.6782\n",
      "Train     - [np.float64(0.5928101503759399), 0.613935685535532, 0.6623302953214565]\n",
      "Validation- [np.float64(0.5907933897243107), 0.6126207408403814, 0.6624807822390849]\n",
      "Test      - [np.float64(0.5897360588972431), 0.6122175349581603, 0.6596643874600437]\n",
      "Epoch 2/100 - Training Loss: 0.6614\n",
      "Train     - [np.float64(0.5732411385606875), 0.6328383515585507, 0.5510162562003055]\n",
      "Validation- [np.float64(0.5719180764411027), 0.6325266831389673, 0.553423659337715]\n",
      "Test      - [np.float64(0.5737586152882206), 0.6333944521300265, 0.5529889324838353]\n",
      "Epoch 3/100 - Training Loss: 0.6551\n",
      "Train     - [np.float64(0.5787837338882922), 0.6429640512234417, 0.5515603516264058]\n",
      "Validation- [np.float64(0.5796130952380952), 0.6435695282554168, 0.5568253706953943]\n",
      "Test      - [np.float64(0.579358552631579), 0.6444830639055221, 0.5527180721767249]\n",
      "Epoch 4/100 - Training Loss: 0.6506\n",
      "Train     - [np.float64(0.5916437298603652), 0.6480599219940586, 0.5787472704822457]\n",
      "Validation- [np.float64(0.5935346177944862), 0.6467971487974207, 0.5839881678329654]\n",
      "Test      - [np.float64(0.5917136591478697), 0.650478328708117, 0.5778342336714974]\n",
      "Epoch 5/100 - Training Loss: 0.6475\n",
      "Train     - [np.float64(0.5956003289473685), 0.6432100626783313, 0.6143465415629935]\n",
      "Validation- [np.float64(0.594141604010025), 0.6407926999158942, 0.6154859000233246]\n",
      "Test      - [np.float64(0.5944353070175439), 0.6446907153266062, 0.613483804030034]\n",
      "Epoch 6/100 - Training Loss: 0.6444\n",
      "Train     - [np.float64(0.5925542091836735), 0.6414313999709944, 0.6124382904529476]\n",
      "Validation- [np.float64(0.5915374373433584), 0.638984964898794, 0.6144290648467634]\n",
      "Test      - [np.float64(0.5909500313283208), 0.6412132534321751, 0.6109886189074604]\n",
      "Epoch 7/100 - Training Loss: 0.6425\n",
      "Train     - [np.float64(0.6127483888292159), 0.6492981045736483, 0.6705240542622223]\n",
      "Validation- [np.float64(0.6071820175438597), 0.6450268224188187, 0.6667421930061519]\n",
      "Test      - [np.float64(0.6110393170426065), 0.6496513644823794, 0.6687687874850042]\n",
      "Epoch 8/100 - Training Loss: 0.6401\n",
      "Train     - [np.float64(0.6151357747046187), 0.6508018299184268, 0.6804646577160515]\n",
      "Validation- [np.float64(0.6097861842105263), 0.6465975623697466, 0.6764709674992249]\n",
      "Test      - [np.float64(0.615875626566416), 0.6517711684313362, 0.6810531273552818]\n",
      "Epoch 9/100 - Training Loss: 0.6379\n",
      "Train     - [np.float64(0.6155511546723953), 0.6594723437906813, 0.6621898002715202]\n",
      "Validation- [np.float64(0.6137805451127819), 0.6556190813637528, 0.662238117908281]\n",
      "Test      - [np.float64(0.6117050438596491), 0.657686233227221, 0.6587595359084397]\n",
      "Epoch 10/100 - Training Loss: 0.6368\n",
      "Train     - [np.float64(0.6187399301825993), 0.6606978590531711, 0.6708981387797687]\n",
      "Validation- [np.float64(0.6155036027568922), 0.6563789774290535, 0.669033532316814]\n",
      "Test      - [np.float64(0.6156210839598998), 0.660173785583132, 0.6674174958283718]\n",
      "Epoch 11/100 - Training Loss: 0.6339\n",
      "Train     - [np.float64(0.6044743555316864), 0.6597102084893544, 0.6468769985667978]\n",
      "Validation- [np.float64(0.6041862468671679), 0.6566094619589822, 0.6487645027823798]\n",
      "Test      - [np.float64(0.6022478070175439), 0.6592653063572171, 0.6446529569918579]\n",
      "Epoch 12/100 - Training Loss: 0.6328\n",
      "Train     - [np.float64(0.6102225429645542), 0.6682107634815371, 0.633859004552852]\n",
      "Validation- [np.float64(0.6071036967418546), 0.6640961155596542, 0.6335209798767215]\n",
      "Test      - [np.float64(0.6079064849624061), 0.6671933296780679, 0.6313609035874245]\n",
      "Epoch 13/100 - Training Loss: 0.6318\n",
      "Train     - [np.float64(0.614095226906552), 0.6638531158999406, 0.6591884906005253]\n",
      "Validation- [np.float64(0.6109805764411027), 0.6594560574073536, 0.657404825611716]\n",
      "Test      - [np.float64(0.6113721804511278), 0.6627234528800805, 0.6565279629907573]\n",
      "Epoch 14/100 - Training Loss: 0.6320\n",
      "Train     - [np.float64(0.6150099019871106), 0.6634196435749746, 0.6609215150876326]\n",
      "Validation- [np.float64(0.6104127506265664), 0.6583592230301998, 0.6585443900036633]\n",
      "Test      - [np.float64(0.6133497807017544), 0.663423282521357, 0.6595989584673341]\n",
      "Epoch 15/100 - Training Loss: 0.6312\n",
      "Train     - [np.float64(0.59625067132116), 0.6648209163812482, 0.6173898476283831]\n",
      "Validation- [np.float64(0.593828320802005), 0.66254391842969, 0.6171241154956414]\n",
      "Test      - [np.float64(0.5931038533834586), 0.6635831633822793, 0.6136677427226737]\n",
      "Epoch 16/100 - Training Loss: 0.6306\n",
      "Train     - [np.float64(0.6085190655209453), 0.6665894150879623, 0.6411885344669165]\n",
      "Validation- [np.float64(0.6050281954887218), 0.66328024290713, 0.6397177646416834]\n",
      "Test      - [np.float64(0.6073973997493735), 0.6656528697510833, 0.6397860553008647]\n",
      "Epoch 17/100 - Training Loss: 0.6305\n",
      "Train     - [np.float64(0.604344287056928), 0.665018206557246, 0.6431449969764147]\n",
      "Validation- [np.float64(0.6018366228070176), 0.6609568941690396, 0.6425333821688579]\n",
      "Test      - [np.float64(0.6021499060150376), 0.6636804814552079, 0.6408219468935553]\n",
      "Epoch 18/100 - Training Loss: 0.6299\n",
      "Train     - [np.float64(0.5959947301288937), 0.6698780056877738, 0.6084893066374372]\n",
      "Validation- [np.float64(0.5919682017543859), 0.6655976273319346, 0.606692818293043]\n",
      "Test      - [np.float64(0.5925947681704261), 0.6684034696876231, 0.6040569256258359]\n",
      "Epoch 19/100 - Training Loss: 0.6289\n",
      "Train     - [np.float64(0.6161133861439313), 0.670414135329503, 0.6603338962453761]\n",
      "Validation- [np.float64(0.6120379072681704), 0.6647095453952465, 0.658202181822987]\n",
      "Test      - [np.float64(0.6122924498746867), 0.668934949641839, 0.6562686858238116]\n",
      "Epoch 20/100 - Training Loss: 0.6287\n",
      "Train     - [np.float64(0.612140004027927), 0.6770488006901262, 0.6392257465820551]\n",
      "Validation- [np.float64(0.608983395989975), 0.6722242651114263, 0.6382055892682272]\n",
      "Test      - [np.float64(0.6088267543859649), 0.6755968762488669, 0.6352413438583743]\n",
      "Epoch 21/100 - Training Loss: 0.6270\n",
      "Train     - [np.float64(0.6121022422126745), 0.6787462330942269, 0.6377426010000542]\n",
      "Validation- [np.float64(0.6074561403508771), 0.6741715779364517, 0.6351639883156329]\n",
      "Test      - [np.float64(0.6093162593984962), 0.6770223495996117, 0.6348901960569086]\n",
      "Epoch 22/100 - Training Loss: 0.6282\n",
      "Train     - [np.float64(0.6080743152524167), 0.6773250216359508, 0.6239333877916498]\n",
      "Validation- [np.float64(0.6039512844611529), 0.6730618837722385, 0.6226248922047182]\n",
      "Test      - [np.float64(0.6049890350877193), 0.67519899285194, 0.6204037832940381]\n",
      "Epoch 23/100 - Training Loss: 0.6268\n",
      "Train     - [np.float64(0.6062113990332976), 0.6748908752928264, 0.628042379623882]\n",
      "Validation- [np.float64(0.602874373433584), 0.6700215421921009, 0.626853849905411]\n",
      "Test      - [np.float64(0.6029918546365914), 0.6728449556040387, 0.6242988848806221]\n",
      "Epoch 24/100 - Training Loss: 0.6264\n",
      "Train     - [np.float64(0.6090225563909775), 0.6733897733633682, 0.6370258078029121]\n",
      "Validation- [np.float64(0.605733082706767), 0.6686310519123082, 0.6359435780066067]\n",
      "Test      - [np.float64(0.6053414786967418), 0.6715602486288907, 0.6332205860752564]\n",
      "Epoch 25/100 - Training Loss: 0.6266\n",
      "Train     - [np.float64(0.618265809613319), 0.677314144370398, 0.6662990137727158]\n",
      "Validation- [np.float64(0.6154057017543859), 0.6721262388092503, 0.6653106515997073]\n",
      "Test      - [np.float64(0.6140938283208021), 0.6755158944080594, 0.6622128016415703]\n",
      "Epoch 26/100 - Training Loss: 0.6262\n",
      "Train     - [np.float64(0.6110533029001074), 0.6767441939060521, 0.6502278519161331]\n",
      "Validation- [np.float64(0.6064967105263158), 0.6715082227658424, 0.6478241101805563]\n",
      "Test      - [np.float64(0.6084743107769424), 0.6751676726159985, 0.6478886233393505]\n",
      "Epoch 27/100 - Training Loss: 0.6262\n",
      "Train     - [np.float64(0.6111791756176155), 0.6789574525276332, 0.6375627017814959]\n",
      "Validation- [np.float64(0.6070449561403509), 0.6736135129543706, 0.6358828682485983]\n",
      "Test      - [np.float64(0.6092379385964912), 0.6773451220660409, 0.6356328090066973]\n",
      "Epoch 28/100 - Training Loss: 0.6252\n",
      "Train     - [np.float64(0.6120686761546724), 0.6810469967657664, 0.6404121961064888]\n",
      "Validation- [np.float64(0.6078869047619048), 0.6762195170957914, 0.638102746562168]\n",
      "Test      - [np.float64(0.6088463345864662), 0.6787813115714192, 0.6370217870493061]\n",
      "Epoch 29/100 - Training Loss: 0.6261\n",
      "Train     - [np.float64(0.6097064648227712), 0.6712641553730608, 0.6479204879344149]\n",
      "Validation- [np.float64(0.6054785401002506), 0.6655725827330854, 0.6457576951969358]\n",
      "Test      - [np.float64(0.6070841165413534), 0.6694959145073548, 0.6455264970334791]\n",
      "Epoch 30/100 - Training Loss: 0.6253\n",
      "Train     - [np.float64(0.611422529538131), 0.678065709884327, 0.6562995910801289]\n",
      "Validation- [np.float64(0.6070841165413534), 0.6730064812876952, 0.6540617673857803]\n",
      "Test      - [np.float64(0.6077694235588973), 0.6759166642856573, 0.6528909325202975]\n",
      "Epoch 31/100 - Training Loss: 0.6250\n",
      "Train     - [np.float64(0.6071890104726101), 0.6789826117126518, 0.6372331317582133]\n",
      "Validation- [np.float64(0.6029722744360902), 0.6733668490235389, 0.6347719733333637]\n",
      "Test      - [np.float64(0.6031484962406015), 0.6764608668216496, 0.6324797410442753]\n",
      "Epoch 32/100 - Training Loss: 0.6241\n",
      "Train     - [np.float64(0.6108896683673469), 0.6839625038868088, 0.641311491213671]\n",
      "Validation- [np.float64(0.6064771303258145), 0.6786968512667693, 0.6386765323073298]\n",
      "Test      - [np.float64(0.6077890037593985), 0.6814878564892369, 0.6381674837989534]\n",
      "Epoch 33/100 - Training Loss: 0.6244\n",
      "Train     - [np.float64(0.6104449180988185), 0.6795060439591931, 0.6392693983606251]\n",
      "Validation- [np.float64(0.6059876253132832), 0.6740730125445715, 0.6367694713596441]\n",
      "Test      - [np.float64(0.6073386591478697), 0.6772695261182653, 0.6363173474885089]\n",
      "Epoch 34/100 - Training Loss: 0.6242\n",
      "Train     - [np.float64(0.6107679914070892), 0.6824711410085824, 0.6426369478876035]\n",
      "Validation- [np.float64(0.6059876253132832), 0.6775817722736863, 0.6401439492202395]\n",
      "Test      - [np.float64(0.6075540413533834), 0.6799420059827823, 0.639528747201834]\n",
      "Epoch 35/100 - Training Loss: 0.6239\n",
      "Train     - [np.float64(0.6144015171858217), 0.6811335650940233, 0.648282219045273]\n",
      "Validation- [np.float64(0.6101190476190477), 0.6759459974650298, 0.645924888784477]\n",
      "Test      - [np.float64(0.6111959586466166), 0.6786289466689042, 0.6452038388778023]\n",
      "Epoch 36/100 - Training Loss: 0.6242\n",
      "Train     - [np.float64(0.6157189849624061), 0.6824590806079229, 0.6512398881645584]\n",
      "Validation- [np.float64(0.610921835839599), 0.6770052571786485, 0.6487040838284209]\n",
      "Test      - [np.float64(0.6136826441102757), 0.6794393832080365, 0.6496036955105495]\n",
      "Epoch 37/100 - Training Loss: 0.6236\n",
      "Train     - [np.float64(0.6216182196562836), 0.6878781772015761, 0.6497525999486341]\n",
      "Validation- [np.float64(0.6167371553884712), 0.682577347624594, 0.6475239029727766]\n",
      "Test      - [np.float64(0.6187734962406015), 0.6848540504340781, 0.6471084893375976]\n",
      "Epoch 38/100 - Training Loss: 0.6234\n",
      "Train     - [np.float64(0.6173679175617616), 0.6874262815661285, 0.638064567388182]\n",
      "Validation- [np.float64(0.6125078320802005), 0.6819047026920582, 0.6353350660102347]\n",
      "Test      - [np.float64(0.613545582706767), 0.6843961931586245, 0.6338807000627803]\n",
      "Epoch 39/100 - Training Loss: 0.6235\n",
      "Train     - [np.float64(0.6099498187432868), 0.6817041505420555, 0.6320528564654544]\n",
      "Validation- [np.float64(0.604499530075188), 0.6760027311287269, 0.6289520978119806]\n",
      "Test      - [np.float64(0.6069470551378446), 0.679279487285502, 0.6291563426585017]\n",
      "Epoch 40/100 - Training Loss: 0.6239\n",
      "Train     - [np.float64(0.6133567736305048), 0.6810537033685446, 0.6360716213206028]\n",
      "Validation- [np.float64(0.6087875939849624), 0.6755826107164166, 0.6339103628181233]\n",
      "Test      - [np.float64(0.6096099624060151), 0.6783968450387728, 0.632406427913974]\n",
      "Epoch 41/100 - Training Loss: 0.6227\n",
      "Train     - [np.float64(0.606375033566058), 0.6818551438744617, 0.6302743776558213]\n",
      "Validation- [np.float64(0.6011904761904762), 0.6760698128536403, 0.6269314540027213]\n",
      "Test      - [np.float64(0.6026785714285714), 0.6792715907061393, 0.6261519784345886]\n",
      "Epoch 42/100 - Training Loss: 0.6232\n",
      "Train     - [np.float64(0.6072519468313641), 0.6798870932746754, 0.6299095806776659]\n",
      "Validation- [np.float64(0.6025023496240601), 0.6741972247966017, 0.6269887648993361]\n",
      "Test      - [np.float64(0.604107926065163), 0.6770869192291926, 0.6269805859778237]\n",
      "Epoch 43/100 - Training Loss: 0.6233\n",
      "Train     - [np.float64(0.6041344991944146), 0.683579387752951, 0.6202425477315529]\n",
      "Validation- [np.float64(0.5986450501253133), 0.6783640930817681, 0.616694452534476]\n",
      "Test      - [np.float64(0.6007792919799498), 0.6799379124959858, 0.616859954322263]\n",
      "Epoch 44/100 - Training Loss: 0.6234\n",
      "Train     - [np.float64(0.6063204887218046), 0.6827528038184119, 0.6332712038282095]\n",
      "Validation- [np.float64(0.6013079573934837), 0.6775313122312867, 0.6298913553347121]\n",
      "Test      - [np.float64(0.6022673872180451), 0.6797200563811723, 0.6288105205315657]\n",
      "Epoch 45/100 - Training Loss: 0.6231\n",
      "Train     - [np.float64(0.6111791756176155), 0.681966952202658, 0.6412329029335718]\n",
      "Validation- [np.float64(0.6075540413533834), 0.6761262437659509, 0.6399540477316159]\n",
      "Test      - [np.float64(0.6085330513784462), 0.6788193110003079, 0.6390191568687968]\n",
      "Epoch 46/100 - Training Loss: 0.6230\n",
      "Train     - [np.float64(0.6058967172395274), 0.6824698026931558, 0.6298912016813896]\n",
      "Validation- [np.float64(0.6012296365914787), 0.676711288600477, 0.6270781994777979]\n",
      "Test      - [np.float64(0.6028547932330827), 0.679490965876587, 0.6270462117263436]\n",
      "Epoch 47/100 - Training Loss: 0.6229\n",
      "Train     - [np.float64(0.6141162056928035), 0.6841319887521282, 0.643527464963377]\n",
      "Validation- [np.float64(0.6093358395989975), 0.6781616812895869, 0.640574633094822]\n",
      "Test      - [np.float64(0.6111567982456141), 0.6810220640197102, 0.640655524677622]\n",
      "Epoch 48/100 - Training Loss: 0.6229\n",
      "Train     - [np.float64(0.603421220461869), 0.6826043573107752, 0.6271480169732403]\n",
      "Validation- [np.float64(0.5983513471177945), 0.6774795761968699, 0.6232606351828472]\n",
      "Test      - [np.float64(0.5994086779448622), 0.6792679215835635, 0.622478885571801]\n",
      "Epoch 49/100 - Training Loss: 0.6230\n",
      "Train     - [np.float64(0.6123078343179377), 0.685336825191059, 0.636687537748622]\n",
      "Validation- [np.float64(0.6070645363408521), 0.6798473644651193, 0.6329607236880825]\n",
      "Test      - [np.float64(0.6083176691729323), 0.6815935155629558, 0.6319784772832012]\n",
      "Epoch 50/100 - Training Loss: 0.6220\n",
      "Train     - [np.float64(0.6016212406015038), 0.6808844996145569, 0.6206342471249856]\n",
      "Validation- [np.float64(0.596687030075188), 0.6753369870117507, 0.618501031061224]\n",
      "Test      - [np.float64(0.5973723370927319), 0.6774594217718146, 0.6155068458584939]\n",
      "Epoch 51/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6119595864661654), 0.6827739735131634, 0.6372708991480865]\n",
      "Validation- [np.float64(0.6071624373433584), 0.6769213113866228, 0.6347892500765374]\n",
      "Test      - [np.float64(0.6089246553884712), 0.6795858930592694, 0.6344825365311758]\n",
      "Epoch 52/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.6001191595059077), 0.6787255856368135, 0.6192833048820864]\n",
      "Validation- [np.float64(0.5942395050125313), 0.6728058009716553, 0.6150182587054677]\n",
      "Test      - [np.float64(0.596921992481203), 0.6753727903547745, 0.6161330926967866]\n",
      "Epoch 53/100 - Training Loss: 0.6225\n",
      "Train     - [np.float64(0.6091358418367347), 0.6828691632654451, 0.6281087475161261]\n",
      "Validation- [np.float64(0.6030114348370927), 0.6771824322380373, 0.6236573316171757]\n",
      "Test      - [np.float64(0.6055568609022557), 0.6795954424513901, 0.6246507897581091]\n",
      "Epoch 54/100 - Training Loss: 0.6233\n",
      "Train     - [np.float64(0.6125847542964554), 0.6838579082446523, 0.638263223755407]\n",
      "Validation- [np.float64(0.6073778195488722), 0.6776935068733934, 0.6350867106039347]\n",
      "Test      - [np.float64(0.6095708020050126), 0.6806003015429307, 0.6354766033569649]\n",
      "Epoch 55/100 - Training Loss: 0.6227\n",
      "Train     - [np.float64(0.6175609223952739), 0.6866894837119266, 0.6480486882262438]\n",
      "Validation- [np.float64(0.613016917293233), 0.6806148177356685, 0.6456241138487294]\n",
      "Test      - [np.float64(0.6150924185463659), 0.6832930624706445, 0.645846312906298]\n",
      "Epoch 56/100 - Training Loss: 0.6224\n",
      "Train     - [np.float64(0.6151357747046187), 0.6862069599841171, 0.6431631803943971]\n",
      "Validation- [np.float64(0.6100798872180451), 0.6806352219399534, 0.6404599856310383]\n",
      "Test      - [np.float64(0.6113330200501254), 0.6825086478222573, 0.639559714319177]\n",
      "Epoch 57/100 - Training Loss: 0.6225\n",
      "Train     - [np.float64(0.6111288265306123), 0.6815443465919109, 0.6408632529843168]\n",
      "Validation- [np.float64(0.6070253759398496), 0.6763159343905493, 0.6387676745401352]\n",
      "Test      - [np.float64(0.607984805764411), 0.677753873934646, 0.6377530095952402]\n",
      "Epoch 58/100 - Training Loss: 0.6222\n",
      "Train     - [np.float64(0.6041764567669173), 0.6816493586329674, 0.6245511782784842]\n",
      "Validation- [np.float64(0.598703790726817), 0.6757878697964067, 0.6203119819042672]\n",
      "Test      - [np.float64(0.6003681077694235), 0.6780198077496135, 0.6200547196322277]\n",
      "Epoch 59/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6123539876476907), 0.6840107372877544, 0.6290583021485273]\n",
      "Validation- [np.float64(0.6062421679197995), 0.678115499785312, 0.6253032980047574]\n",
      "Test      - [np.float64(0.6100407268170426), 0.6804166488675595, 0.6267386252181137]\n",
      "Epoch 60/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6048897354994629), 0.6842906427640053, 0.61924587498224]\n",
      "Validation- [np.float64(0.5987233709273183), 0.6782774245228779, 0.6153904001902075]\n",
      "Test      - [np.float64(0.6013471177944862), 0.6806398945252186, 0.6157758288672777]\n",
      "Epoch 61/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.608640742481203), 0.6844398982626141, 0.6245168852157144]\n",
      "Validation- [np.float64(0.6032855576441103), 0.6786092488641022, 0.6208673083880312]\n",
      "Test      - [np.float64(0.6054002192982456), 0.6809635363806685, 0.6211188589844726]\n",
      "Epoch 62/100 - Training Loss: 0.6222\n",
      "Train     - [np.float64(0.6093414339419978), 0.6809643896301709, 0.6386069633166785]\n",
      "Validation- [np.float64(0.604421209273183), 0.6749098340138069, 0.6354861545287853]\n",
      "Test      - [np.float64(0.6064967105263158), 0.6775654205867826, 0.6358545567422533]\n",
      "Epoch 63/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.6115735767991407), 0.6876887895765946, 0.6295847661245987]\n",
      "Validation- [np.float64(0.6062813283208021), 0.6816522517028267, 0.6268421886793014]\n",
      "Test      - [np.float64(0.6066729323308271), 0.6839589826964972, 0.624569962947038]\n",
      "Epoch 64/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6135581699785178), 0.6880611509992554, 0.6303474457043992]\n",
      "Validation- [np.float64(0.6076127819548872), 0.6819913306274263, 0.6268034933616482]\n",
      "Test      - [np.float64(0.6087484335839599), 0.6844823779838275, 0.6253888955894102]\n",
      "Epoch 65/100 - Training Loss: 0.6219\n",
      "Train     - [np.float64(0.6149553571428571), 0.6883879045328751, 0.6397719477980526]\n",
      "Validation- [np.float64(0.6098840852130326), 0.6826132493122277, 0.6376225916012405]\n",
      "Test      - [np.float64(0.611078477443609), 0.6844412571712553, 0.6361382954802913]\n",
      "Epoch 66/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.606811392320086), 0.6873545163549843, 0.6236741090980912]\n",
      "Validation- [np.float64(0.6011121553884712), 0.681295131552653, 0.6201211181552727]\n",
      "Test      - [np.float64(0.6027177318295739), 0.6835369808614221, 0.6188225048592263]\n",
      "Epoch 67/100 - Training Loss: 0.6218\n",
      "Train     - [np.float64(0.6118714755639098), 0.6861401432450366, 0.6385133089356801]\n",
      "Validation- [np.float64(0.6075344611528822), 0.6801393895306123, 0.6365348080580112]\n",
      "Test      - [np.float64(0.6081022869674185), 0.6822547250949498, 0.6349019423994527]\n",
      "Epoch 68/100 - Training Loss: 0.6216\n",
      "Train     - [np.float64(0.6120896549409237), 0.6847824809304977, 0.633759856909257]\n",
      "Validation- [np.float64(0.607358239348371), 0.6793323779158913, 0.6313235667331094]\n",
      "Test      - [np.float64(0.6088071741854637), 0.680899467169907, 0.630561264752405]\n",
      "Epoch 69/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.6118630840494093), 0.6834567621233683, 0.6434610605702024]\n",
      "Validation- [np.float64(0.6074561403508771), 0.6779825384813718, 0.6411200478534177]\n",
      "Test      - [np.float64(0.6083372493734336), 0.6797902227561807, 0.640300192997427]\n",
      "Epoch 70/100 - Training Loss: 0.6215\n",
      "Train     - [np.float64(0.60687432867884), 0.6814688827087096, 0.6336848833981572]\n",
      "Validation- [np.float64(0.6024044486215538), 0.6756696648115446, 0.630931220618304]\n",
      "Test      - [np.float64(0.6033834586466166), 0.6777265741445926, 0.6297519248134411]\n",
      "Epoch 71/100 - Training Loss: 0.6222\n",
      "Train     - [np.float64(0.6083931928034372), 0.6840238190758241, 0.6304964158801755]\n",
      "Validation- [np.float64(0.6033247180451128), 0.678111196310099, 0.6278045922589255]\n",
      "Test      - [np.float64(0.6047736528822055), 0.6804090613097485, 0.626778756221587]\n",
      "Epoch 72/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6052925281954887), 0.682224083158058, 0.6296737686987569]\n",
      "Validation- [np.float64(0.6001331453634086), 0.6764740006709626, 0.6262725435456163]\n",
      "Test      - [np.float64(0.6010729949874687), 0.6791427333493245, 0.6246822483726892]\n",
      "Epoch 73/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6038491877013964), 0.6804632415977971, 0.6224416993212647]\n",
      "Validation- [np.float64(0.5992128759398496), 0.6741839318250593, 0.6195169602591223]\n",
      "Test      - [np.float64(0.5996632205513784), 0.677319325853986, 0.6174335119491356]\n",
      "Epoch 74/100 - Training Loss: 0.6213\n",
      "Train     - [np.float64(0.6143343850698174), 0.6859853256608909, 0.6434561102345359]\n",
      "Validation- [np.float64(0.6100407268170426), 0.6806224524975464, 0.6410783056510901]\n",
      "Test      - [np.float64(0.6117637844611529), 0.6817995800463295, 0.6413728796529111]\n",
      "Epoch 75/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.6086155679377014), 0.6825582637893071, 0.6297496212299497]\n",
      "Validation- [np.float64(0.6040100250626567), 0.6765870471601607, 0.626944477925599]\n",
      "Test      - [np.float64(0.6054589598997494), 0.6795586378288829, 0.6267264777881312]\n",
      "Epoch 76/100 - Training Loss: 0.6216\n",
      "Train     - [np.float64(0.6060100026852846), 0.6847352995509273, 0.6267605592910115]\n",
      "Validation- [np.float64(0.6015625), 0.6787174999795205, 0.6245843883475872]\n",
      "Test      - [np.float64(0.6024436090225563), 0.6810029522369452, 0.6231255472935828]\n",
      "Epoch 77/100 - Training Loss: 0.6220\n",
      "Train     - [np.float64(0.6162896079484426), 0.6882072047542416, 0.6372498045850542]\n",
      "Validation- [np.float64(0.6105889724310777), 0.6821326647127033, 0.6343587922126225]\n",
      "Test      - [np.float64(0.6114700814536341), 0.6845868120624179, 0.6324112824339103]\n",
      "Epoch 78/100 - Training Loss: 0.6227\n",
      "Train     - [np.float64(0.6138518729860365), 0.6864254338704315, 0.6407745942227872]\n",
      "Validation- [np.float64(0.6094337406015038), 0.680714400560756, 0.6385211580493436]\n",
      "Test      - [np.float64(0.6107456140350878), 0.6825520697223902, 0.6378844189394076]\n",
      "Epoch 79/100 - Training Loss: 0.6217\n",
      "Train     - [np.float64(0.6066729323308271), 0.6827331154887026, 0.6260721441840792]\n",
      "Validation- [np.float64(0.6022086466165414), 0.6771620419130705, 0.6237288515041176]\n",
      "Test      - [np.float64(0.602952694235589), 0.6791936477762115, 0.6219534418832966]\n",
      "Epoch 80/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6123623791621912), 0.6833006027694966, 0.6381810278708494]\n",
      "Validation- [np.float64(0.6074953007518797), 0.6772683286556216, 0.6353001371859497]\n",
      "Test      - [np.float64(0.6088659147869674), 0.6797488228550554, 0.6349396616158919]\n",
      "Epoch 81/100 - Training Loss: 0.6230\n",
      "Train     - [np.float64(0.6140784438775511), 0.6855176227047687, 0.6406213865502138]\n",
      "Validation- [np.float64(0.6094924812030075), 0.6796552778999831, 0.6381712830969832]\n",
      "Test      - [np.float64(0.6111763784461153), 0.6821029127017646, 0.637896765271201]\n",
      "Epoch 82/100 - Training Loss: 0.6220\n",
      "Train     - [np.float64(0.606437969924812), 0.6857035102321263, 0.6257031508391624]\n",
      "Validation- [np.float64(0.6013862781954887), 0.6796538334185771, 0.6227712593226692]\n",
      "Test      - [np.float64(0.6026785714285714), 0.6820077494419394, 0.6216545231606203]\n",
      "Epoch 83/100 - Training Loss: 0.6225\n",
      "Train     - [np.float64(0.6062449650912997), 0.68090805315325, 0.6320674921781434]\n",
      "Validation- [np.float64(0.6007792919799498), 0.6749405800804731, 0.6285119150127765]\n",
      "Test      - [np.float64(0.603187656641604), 0.6775244953312767, 0.6289619170800022]\n",
      "Epoch 84/100 - Training Loss: 0.6218\n",
      "Train     - [np.float64(0.6058337808807733), 0.6843205312939981, 0.6257875859858528]\n",
      "Validation- [np.float64(0.6001527255639098), 0.678657190800872, 0.6222814886439463]\n",
      "Test      - [np.float64(0.6011513157894737), 0.6806737151549022, 0.6204354828744959]\n",
      "Epoch 85/100 - Training Loss: 0.6215\n",
      "Train     - [np.float64(0.6138602645005371), 0.6846950541870156, 0.6432586729654745]\n",
      "Validation- [np.float64(0.6088071741854637), 0.6788467072434895, 0.6404338075142814]\n",
      "Test      - [np.float64(0.609766604010025), 0.6808781545963134, 0.6389288325475116]\n",
      "Epoch 86/100 - Training Loss: 0.6222\n",
      "Train     - [np.float64(0.611468682867884), 0.6858843459764173, 0.6362570801053482]\n",
      "Validation- [np.float64(0.6057918233082706), 0.6799694606555897, 0.632683943913359]\n",
      "Test      - [np.float64(0.6082001879699248), 0.6822673185822012, 0.6328978460895863]\n",
      "Epoch 87/100 - Training Loss: 0.6217\n",
      "Train     - [np.float64(0.6118253222341569), 0.6856143707906455, 0.6340272875638868]\n",
      "Validation- [np.float64(0.6059876253132832), 0.6795362888411499, 0.6301697540289334]\n",
      "Test      - [np.float64(0.6090421365914787), 0.6817170476460128, 0.6312551314274716]\n",
      "Epoch 88/100 - Training Loss: 0.6215\n",
      "Train     - [np.float64(0.6120560888829216), 0.6861429510511661, 0.6343116220675131]\n",
      "Validation- [np.float64(0.6056351817042607), 0.6800659000545233, 0.6302985501592154]\n",
      "Test      - [np.float64(0.6080043859649122), 0.6826859846247003, 0.6299966315706496]\n",
      "Epoch 89/100 - Training Loss: 0.6219\n",
      "Train     - [np.float64(0.6092113654672395), 0.6836596271941708, 0.6389633826726326]\n",
      "Validation- [np.float64(0.6039512844611529), 0.6778402480064678, 0.635761876305578]\n",
      "Test      - [np.float64(0.6052631578947368), 0.6797895503394972, 0.6350258522151382]\n",
      "Epoch 90/100 - Training Loss: 0.6218\n",
      "Train     - [np.float64(0.6088673133727175), 0.6846358426505643, 0.6245242990814941]\n",
      "Validation- [np.float64(0.6036380012531328), 0.6785785938552525, 0.6213622382157714]\n",
      "Test      - [np.float64(0.6061051065162907), 0.6811790250473534, 0.621373973924372]\n",
      "Epoch 91/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.6125931458109559), 0.6864047410442556, 0.6363775796614005]\n",
      "Validation- [np.float64(0.6068491541353384), 0.6804308852904823, 0.632962062403008]\n",
      "Test      - [np.float64(0.6090225563909775), 0.6827493655665173, 0.6327077948683061]\n",
      "Epoch 92/100 - Training Loss: 0.6217\n",
      "Train     - [np.float64(0.6152280813641245), 0.6863714528963429, 0.642365857773955]\n",
      "Validation- [np.float64(0.6102756892230576), 0.68043491505825, 0.6399161809293721]\n",
      "Test      - [np.float64(0.6119595864661654), 0.6826633636345076, 0.639386943940401]\n",
      "Epoch 93/100 - Training Loss: 0.6222\n",
      "Train     - [np.float64(0.6078938976906552), 0.6795417606695509, 0.632333093403865]\n",
      "Validation- [np.float64(0.6023652882205514), 0.6736451880597678, 0.6283418048678246]\n",
      "Test      - [np.float64(0.6046757518796992), 0.6765521046032993, 0.6289227504519319]\n",
      "Epoch 94/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6082924946294307), 0.6843684121335838, 0.6280672961506433]\n",
      "Validation- [np.float64(0.6037163220551378), 0.6784796424969485, 0.6262599228949756]\n",
      "Test      - [np.float64(0.6048519736842105), 0.6805802577838251, 0.6247750675915228]\n",
      "Epoch 95/100 - Training Loss: 0.6221\n",
      "Train     - [np.float64(0.6108435150375939), 0.6857623664678705, 0.6288346371455269]\n",
      "Validation- [np.float64(0.6052631578947368), 0.6800905747235229, 0.6255057632170692]\n",
      "Test      - [np.float64(0.6068883145363408), 0.6821361217874087, 0.624660434892361]\n",
      "Epoch 96/100 - Training Loss: 0.6222\n",
      "Train     - [np.float64(0.606500906283566), 0.6809435206844039, 0.6336071941330809]\n",
      "Validation- [np.float64(0.6013471177944862), 0.6754675128844672, 0.630188832398352]\n",
      "Test      - [np.float64(0.6024436090225563), 0.6771152178528126, 0.6288970761486619]\n",
      "Epoch 97/100 - Training Loss: 0.6220\n",
      "Train     - [np.float64(0.6088337473147154), 0.6818643597975385, 0.6327659693786198]\n",
      "Validation- [np.float64(0.6036771616541353), 0.675534882169234, 0.62937178788491]\n",
      "Test      - [np.float64(0.6059288847117794), 0.6784626153788438, 0.6297769271820653]\n",
      "Epoch 98/100 - Training Loss: 0.6226\n",
      "Train     - [np.float64(0.6100127551020408), 0.6853022715434719, 0.6342343046825645]\n",
      "Validation- [np.float64(0.6057526629072681), 0.6792701365472243, 0.632126222603829]\n",
      "Test      - [np.float64(0.6066337719298246), 0.6819137253269008, 0.6310141960306609]\n",
      "Epoch 99/100 - Training Loss: 0.6223\n",
      "Train     - [np.float64(0.6068953074650913), 0.6833388559120324, 0.6281726526573695]\n",
      "Validation- [np.float64(0.6012883771929824), 0.6773698153468842, 0.6245080206456223]\n",
      "Test      - [np.float64(0.603187656641604), 0.6798164320573539, 0.6243981306457462]\n",
      "Epoch 100/100 - Training Loss: 0.6215\n",
      "Train     - [np.float64(0.6118379095059077), 0.6826821170501025, 0.6428589919488737]\n",
      "Validation- [np.float64(0.6072994987468672), 0.6774651430958731, 0.6397651982056788]\n",
      "Test      - [np.float64(0.6092183583959899), 0.6788177654022929, 0.6406491513494503]\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T18:59:01.084671Z",
     "start_time": "2025-10-06T18:59:00.598565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    outputs = net(inputs)  # shape: (402, 2)\n",
    "    probs = torch.sigmoid(outputs)  # convert logits â†’ probabilities [0,1]\n",
    "    preds = (probs > 0.5).int()     # threshold at 0.5 â†’ class 0 or 1\n",
    "\n",
    "# Move back to CPU for viewing\n",
    "pred_classes = preds.cpu().numpy()\n",
    "true_classes = y_test.astype(int)\n",
    "\n",
    "# Print some examples\n",
    "for i in range(10):\n",
    "    print(f\"Sample {i+1}: Predicted [Valence, Arousal] = {pred_classes[i]}, True = {true_classes[i]}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted [Valence, Arousal] = [1 0], True = [0 1]\n",
      "Sample 2: Predicted [Valence, Arousal] = [1 1], True = [0 1]\n",
      "Sample 3: Predicted [Valence, Arousal] = [1 0], True = [1 1]\n",
      "Sample 4: Predicted [Valence, Arousal] = [1 1], True = [1 1]\n",
      "Sample 5: Predicted [Valence, Arousal] = [1 0], True = [1 1]\n",
      "Sample 6: Predicted [Valence, Arousal] = [0 1], True = [1 0]\n",
      "Sample 7: Predicted [Valence, Arousal] = [0 0], True = [1 1]\n",
      "Sample 8: Predicted [Valence, Arousal] = [1 1], True = [1 0]\n",
      "Sample 9: Predicted [Valence, Arousal] = [0 1], True = [1 1]\n",
      "Sample 10: Predicted [Valence, Arousal] = [1 1], True = [1 1]\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-10-06T18:59:01.180344Z",
     "start_time": "2025-10-06T18:59:01.167007Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
